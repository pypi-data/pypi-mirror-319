import hashlib
import json
import os
import re
import time
from datetime import datetime
import requests
import retrying
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from gne import GeneralNewsExtractor
from htmldate import find_date
from loguru import logger
from urllib.parse import urljoin
from lxml import etree
from lxparse import LxParse


headers = {'user-agent':  str(UserAgent().random)}

def get_page_info(url, page_param=None, step=1, first_num=1, mode='direct', max_attempts=10, use_cache=False, cache_file=None, proxy=None,stop_max_attempt_number=3,sleep=1):
    """
    è·å–é¡µé¢ä¿¡æ¯ï¼Œæ”¯æŒç›´æ¥è§£æå’ŒäºŒåˆ†æŸ¥æ‰¾ä¸¤ç§æ¨¡å¼
    :param url: åˆå§‹URL
    :param page_param: é¡µç å‚æ•°å
    :param step: é¡µç æ­¥é•¿,å¦‚æœé¡µç ä¸æ˜¯è¿ç»­çš„ï¼Œå¯ä»¥è®¾ç½®æ­¥é•¿
    :param first_num: èµ·å§‹é¡µç 
    :param mode: æ¨¡å¼é€‰æ‹©ï¼Œ'direct'ï¼ˆç›´æ¥è§£æï¼‰æˆ– 'binary'ï¼ˆäºŒåˆ†æŸ¥æ‰¾ï¼‰
    :param max_attempts: äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å°è¯•æ¬¡æ•°
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤å…³é—­
    :param cache_file: ç¼“å­˜æ–‡ä»¶å
    :param proxy: ä»£ç†è®¾ç½®
    :return: é¡µé¢é“¾æ¥åˆ—è¡¨, æ€»é¡µæ•°
    :stop_max_attempt_number: é‡è¯•æ¬¡æ•°
    :sleep: é‡è¯•é—´éš”

    """
    headers = {'user-agent': str(UserAgent().random)}
    if not cache_file:
        cache_file = 'total_pages_cache.json'
    def extract_base_url(url, page_param):
        if page_param.startswith('/'):
            pattern = rf'(.*{page_param})\d+'
        else:
            pattern = rf'(.*[?&]{page_param}=)\d+'
        match = re.search(pattern, url)
        return match.group(1) if match else url.split('?')[0]

    @retrying.retry(wait_fixed=2000, stop_max_attempt_number=3)
    def get_response(url, params=None, data=None, proxy=None):
        try:
            if data == None:
                if params == None:
                    response = requests.get(url, headers=headers, proxies=proxy, timeout=10)
                else:
                    response = requests.get(url, headers=headers, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
            else:
                if params == None:
                    response = requests.post(url, headers=headers, data=data, proxies=proxy, timeout=10)
                else:
                    response = requests.post(url, headers=headers, data=data, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
        except Exception as e:
            logger.info(f'æŠ“å–å¤±è´¥,é‡æ–°æŠ“å–ï¼š{url}.{e}')
            raise

    # ç¼“å­˜å¤„ç†
    def load_cache():
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_cache(cache):
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=4)

    # æ£€æŸ¥ç¼“å­˜
    if use_cache:
        cache = load_cache()
        base_url=extract_base_url(url, page_param)
        cached_data = cache.get(base_url) or cache.get(url)
        if cached_data:
            logger.success(f"ä»ç¼“å­˜ä¸­è¯»å–: \nbase_url={base_url}\næ€»é¡µæ•°={cached_data['total_pages']}\né“¾æ¥ï¼š{cached_data['all_pages_link'][:3]}...")
            return cached_data['all_pages_link'],cached_data['total_pages'],
        else:
            logger.info("ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®")

    if mode == 'direct':
        # ç›´æ¥è§£ææ¨¡å¼
        response =get_response(url,proxy=proxy).text
        body = etree.HTML(response)
        page_url = body.xpath("//*[text()='å°¾é¡µ' or text()='æœ«é¡µ']/@href")

        if not page_url:
            logger.info("æœªæ‰¾åˆ°å°¾é¡µé“¾æ¥")
            return [url], 1

        full_url = urljoin(url, page_url[0])

        # å¤„ç†ä¸åŒå‚æ•°æ¨¡å¼
        if page_param:
            pattern = rf'(.*{page_param})(\d+)' if page_param.startswith('/') else rf'(.*[?&]{page_param}=)(\d+)'
            match = re.search(pattern, full_url)
            if match:
                base_path = match.group(1)
                last_page_number = match.group(2)
                all_pages_link = [
                    f"{base_path}{(int(last_page_number) - i) * step + first_num}"
                    for i in range(int(last_page_number) - 1, -1, -1)
                ]
                logger.success(f"åŸºæœ¬è·¯å¾„: {base_path}, é¡µç : {last_page_number}")
                return all_pages_link, last_page_number

        # é»˜è®¤å¤„ç†é€»è¾‘
        match = re.match(r'(.*/)([^/]*?_)?(\d+)(_\d+)?(\.html)?', full_url)
        if match:
            base_path = f"{match.group(1)}{match.group(2) or ''}{match.group(3)}_{{}}{match.group(5) or ''}"
            last_page_number = match.group(4)[1:] if match.group(4) else '1'
            all_pages_link = [
                base_path.format(page) for page in range(2, int(last_page_number) + 1)
            ]
            all_pages_link.append(url)
            logger.success(f"\nâœ… åŸºæœ¬è·¯å¾„: {base_path}, é¡µç : {last_page_number}")

            if use_cache:
                cache = load_cache()
                cache['base_url'] = {
                    'total_pages': last_page_number,
                    'all_pages_link': all_pages_link
                }
                save_cache(cache)
                logger.success(f"ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
            return all_pages_link, last_page_number

        logger.info("æœªèƒ½åŒ¹é…åˆ°æœ‰æ•ˆçš„è·¯å¾„")
        return [url], 1

    else:

        # äºŒåˆ†æŸ¥æ‰¾æ¨¡å¼
        def is_page_valid(page_url):
            try:
                response = requests.get(page_url, headers=headers, proxies=proxy, timeout=10)
                logger.info(f"ğŸ“Šæ˜¯å¦æ˜¯æœ€åä¸€é¡µå‘¢?: {page_url}")
                return response.status_code == 200
            except requests.RequestException:
                return False



        def get_page_url(base_url, page_num, page_param):
            return f"{base_url}{page_num}" if page_param.startswith('/') else f"{base_url}{page_num}"

        # æå–base_url
        base_url = extract_base_url(url, page_param)

        # åŠ¨æ€æ‰©å±•èŒƒå›´
        left, right = 1, 100
        attempts = 0
        while attempts < max_attempts:
            attempts += 1
            if is_page_valid(get_page_url(base_url, right, page_param)):
                left = right
                right *= 2
            else:
                break

        # äºŒåˆ†æŸ¥æ‰¾
        while left < right:
            mid = (left + right) // 2
            if is_page_valid(get_page_url(base_url, mid, page_param)):
                left = mid + 1
            else:
                right = mid
            time.sleep(sleep)

        total_pages = left - 1
        all_pages_link = [get_page_url(base_url, i, page_param) for i in range(1, total_pages + 1)]

        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            cache = load_cache()
            cache[f'{base_url}'] = {
                'total_pages': total_pages,
                'all_pages_link': all_pages_link
            }
            save_cache(cache)
            logger.success(f"ğŸ‰æœ€åä¸€é¡µ:{get_page_url(base_url, right, page_param)}")
            logger.success(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}ï¼Œæ€»é¡µæ•°: {total_pages}")


        return all_pages_link, total_pages




def get_links(url,xpath=None,proxy=None):
    @retrying.retry(wait_fixed=2000, stop_max_attempt_number=3)
    def get_response(url, params=None, data=None, proxy=None):
        try:
            if data == None:
                if params == None:
                    response = requests.get(url, headers=headers, proxies=proxy, timeout=10)
                else:
                    response = requests.get(url, headers=headers, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
            else:
                if params == None:
                    response = requests.post(url, headers=headers, data=data, proxies=proxy, timeout=10)
                else:
                    response = requests.post(url, headers=headers, data=data, params=params, proxies=proxy, timeout=10)
                if response.status_code == 200:
                    response.encoding = response.apparent_encoding
                    return response
        except Exception as e:
            logger.info(f'æŠ“å–å¤±è´¥,é‡æ–°æŠ“å–ï¼š{url}.{e}')
            raise


    try:
            lx = LxParse()
            response = get_response(url).text
            detail_url_list = lx.parse_list(response, article_nums=4, xpath_list=xpath)
            urls = [lx.parse_domain(detail_url, url) for detail_url in detail_url_list]
            if len(urls) > 0:
                logger.success(f"url:{url}ï¼›è§£æå‡ºé“¾æ¥{len(urls)}æ¡")
                return urls
            else:
                logger.error(f"url:{url}ï¼›æœªè§£æåˆ°é“¾æ¥,å¯ä¼ å…¥xpath")
                return []
    except  Exception as e:
        logger.error(f'è§£æå¤±è´¥{e}')
        pass




def get_article(url,proxy=None,parsing_mode='gne'):
        def lx_response(res):
            lx = LxParse()
            data = lx.parse_detail(res)
            if data is not None:
                return data
            else:
                logger.debug('è¿”å›å€¼ä¸ºNone')

        def clean_text_bs(html_content):
            # ä½¿ç”¨ BeautifulSoup å»é™¤ HTML æ ‡ç­¾
            soup = BeautifulSoup(html_content, 'html.parser')
            return soup.get_text()

        # ç¤ºä¾‹

        def clean_text_regex(text):
            return re.sub(r'\s+', ' ', text).strip()
        try:
            dit = {}
            response = requests.get(url, headers=headers, proxies=proxy, timeout=10, verify=False)
            response.encoding = response.apparent_encoding
            response = response.text
            extractor = GeneralNewsExtractor()
            result_gne = extractor.extract(response)

            result = lx_response(response)
            _id = hashlib.md5(url.encode('utf-8')).hexdigest()
            dit['_id'] = _id
            dit['url'] = url
            if parsing_mode == 'lx':
                dit['content'] = clean_text_regex(result.get('content_format', ''))
            else:
                dit['content'] = clean_text_bs(clean_text_regex(result_gne.get('content', '')))
            dit['title'] = result.get('title', '')
            dit['updateTime'] = str(datetime.now())[:19]
            dit['addDateTime'] = str(datetime.now())[:19]
            dit['publish_time'] = find_date(response)
            if len(dit['content']) > 0:
                return dit
            else:
                logger.error(f'å†…å®¹ä¸ºç©º:{url}')
                return None
        except Exception as e:
            print(f'get_info å‡ºé”™å•¦ï¼ï¼ï¼ï¼-ã€‹{e}')
            return None

