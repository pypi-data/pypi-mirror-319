"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
import dateutil.parser
from enum import Enum
from orq_rc_poc2.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from orq_rc_poc2.utils import FieldMetadata, QueryParamMetadata, RequestMetadata
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypedDict


class GetAllPromptTemplatesFiltersPromptTemplatesRequestType(str, Enum):
    STRING_ARRAY = "string_array"


class FiltersOperator(str, Enum):
    IN = "in"


class FourTypedDict(TypedDict):
    type: GetAllPromptTemplatesFiltersPromptTemplatesRequestType
    operator: FiltersOperator
    values: List[str]
    path: str


class Four(BaseModel):
    type: GetAllPromptTemplatesFiltersPromptTemplatesRequestType

    operator: FiltersOperator

    values: List[str]

    path: str


class GetAllPromptTemplatesFiltersPromptTemplatesType(str, Enum):
    STRING = "string"


class Operator(str, Enum):
    CONTAINS = "contains"
    EQUALS = "equals"


class Filters3TypedDict(TypedDict):
    type: GetAllPromptTemplatesFiltersPromptTemplatesType
    operator: Operator
    value: str
    path: str


class Filters3(BaseModel):
    type: GetAllPromptTemplatesFiltersPromptTemplatesType

    operator: Operator

    value: str

    path: str


class GetAllPromptTemplatesFiltersType(str, Enum):
    SEARCH = "search"


class Filters2TypedDict(TypedDict):
    type: GetAllPromptTemplatesFiltersType
    value: str
    search_paths: List[str]


class Filters2(BaseModel):
    type: GetAllPromptTemplatesFiltersType

    value: str

    search_paths: Annotated[List[str], pydantic.Field(alias="searchPaths")]


class FiltersType(str, Enum):
    ID = "id"


class Filters1TypedDict(TypedDict):
    type: FiltersType
    id: str
    path: str


class Filters1(BaseModel):
    type: FiltersType

    id: str

    path: str


FiltersTypedDict = Union[
    Filters1TypedDict, Filters2TypedDict, Filters3TypedDict, FourTypedDict
]


Filters = Union[Filters1, Filters2, Filters3, Four]


class GetAllPromptTemplatesRequestBodyTypedDict(TypedDict):
    filters: List[FiltersTypedDict]


class GetAllPromptTemplatesRequestBody(BaseModel):
    filters: List[Filters]


class GetAllPromptTemplatesRequestTypedDict(TypedDict):
    page: NotRequired[str]
    limit: NotRequired[str]
    request_body: NotRequired[GetAllPromptTemplatesRequestBodyTypedDict]


class GetAllPromptTemplatesRequest(BaseModel):
    page: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None

    limit: Annotated[
        Optional[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = None

    request_body: Annotated[
        Optional[GetAllPromptTemplatesRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


class GetAllPromptTemplatesOwnerPromptTemplates2(str, Enum):
    VENDOR = "vendor"


GetAllPromptTemplatesItemsOwnerTypedDict = Union[
    str, GetAllPromptTemplatesOwnerPromptTemplates2
]


GetAllPromptTemplatesItemsOwner = Union[str, GetAllPromptTemplatesOwnerPromptTemplates2]


class GetAllPromptTemplatesItemsPromptTemplatesModelType(str, Enum):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class GetAllPromptTemplatesItemsPromptTemplatesFormat(str, Enum):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesItemsPromptTemplatesQuality(str, Enum):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyType(
    str, Enum
):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2TypedDict(TypedDict):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONType(
    str, Enum
):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponseJSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponseJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse1TypedDict(TypedDict):
    type: (
        GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONType
    )
    json_schema: (
        GetAllPromptTemplatesResponseFormatPromptTemplatesResponseJSONSchemaTypedDict
    )


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse1(BaseModel):
    type: (
        GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONType
    )

    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponseJSONSchema


GetAllPromptTemplatesItemsPromptTemplatesResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2TypedDict,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse1TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptTemplatesItemsPromptTemplatesResponseFormat = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse1,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class GetAllPromptTemplatesItemsPromptTemplatesPhotoRealVersion(str, Enum):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class GetAllPromptTemplatesItemsPromptTemplatesEncodingFormat(str, Enum):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class GetAllPromptTemplatesItemsPromptTemplatesModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptTemplatesItemsPromptTemplatesResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesPhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesEncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptTemplatesItemsPromptTemplatesModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptTemplatesItemsPromptTemplatesFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptTemplatesItemsPromptTemplatesQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptTemplatesItemsPromptTemplatesResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptTemplatesItemsPromptTemplatesPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesProvider(str, Enum):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class GetAllPromptTemplatesItemsPromptTemplatesRole(str, Enum):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyType(
    str, Enum
):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2PromptTemplatesResponseImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponseImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: (
        GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyType
    )
    image_url: GetAllPromptTemplates2PromptTemplatesResponseImageURLTypedDict


class GetAllPromptTemplates2PromptTemplatesResponse2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: (
        GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyType
    )

    image_url: GetAllPromptTemplates2PromptTemplatesResponseImageURL


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONType(str, Enum):
    TEXT = "text"


class GetAllPromptTemplates2PromptTemplatesResponse1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONType
    text: str


class GetAllPromptTemplates2PromptTemplatesResponse1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONType

    text: str


GetAllPromptTemplatesContentPromptTemplatesResponse2TypedDict = Union[
    GetAllPromptTemplates2PromptTemplatesResponse1TypedDict,
    GetAllPromptTemplates2PromptTemplatesResponse2TypedDict,
]


GetAllPromptTemplatesContentPromptTemplatesResponse2 = Union[
    GetAllPromptTemplates2PromptTemplatesResponse1,
    GetAllPromptTemplates2PromptTemplatesResponse2,
]


GetAllPromptTemplatesItemsPromptTemplatesContentTypedDict = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplatesResponse2TypedDict]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptTemplatesItemsPromptTemplatesContent = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplatesResponse2]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Type(str, Enum):
    FUNCTION = "function"


class GetAllPromptTemplatesItemsPromptTemplatesFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesToolCallsTypedDict(TypedDict):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200Type
    function: GetAllPromptTemplatesItemsPromptTemplatesFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptTemplatesItemsPromptTemplatesToolCalls(BaseModel):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200Type

    function: GetAllPromptTemplatesItemsPromptTemplatesFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptTemplatesItemsPromptTemplatesMessagesTypedDict(TypedDict):
    role: GetAllPromptTemplatesItemsPromptTemplatesRole
    r"""The role of the prompt message"""
    content: GetAllPromptTemplatesItemsPromptTemplatesContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[GetAllPromptTemplatesItemsPromptTemplatesToolCallsTypedDict]
    ]


class GetAllPromptTemplatesItemsPromptTemplatesMessages(BaseModel):
    role: GetAllPromptTemplatesItemsPromptTemplatesRole
    r"""The role of the prompt message"""

    content: GetAllPromptTemplatesItemsPromptTemplatesContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptTemplatesItemsPromptTemplatesToolCalls]] = (
        None
    )


class GetAllPromptTemplatesItemsPromptTemplatesPromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptTemplatesItemsPromptTemplatesMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesPromptConfig(BaseModel):
    messages: List[GetAllPromptTemplatesItemsPromptTemplatesMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptTemplatesItemsPromptTemplatesModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptTemplatesItemsPromptTemplatesProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelType(
    str, Enum
):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyFormat(
    str, Enum
):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONQuality(
    str, Enum
):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType(
    str, Enum
):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody2(
    BaseModel
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type(
    str, Enum
):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyJSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyJSONSchema(
    BaseModel
):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody1TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type
    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyJSONSchemaTypedDict


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody1(
    BaseModel
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type

    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyJSONSchema


GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody1TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseFormat = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody2,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBody1,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPhotoRealVersion(
    str, Enum
):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONEncodingFormat(
    str, Enum
):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelParametersTypedDict(
    TypedDict
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyFormat
    ]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONQuality
    ]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseFormatTypedDict
        ]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONEncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelParameters(
    BaseModel
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyFormat
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONQuality
    ] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseFormat
        ],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPhotoRealVersion
        ],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONProvider(
    str, Enum
):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONRole(
    str, Enum
):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType(
    str, Enum
):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyImageURLTypedDict(
    TypedDict
):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyImageURL(
    BaseModel
):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict(
    TypedDict
):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType
    image_url: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyImageURLTypedDict


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody2(
    BaseModel
):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3VersionsType

    image_url: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyImageURL


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type(
    str, Enum
):
    TEXT = "text"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody1TypedDict(
    TypedDict
):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type
    text: str


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody1(
    BaseModel
):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems3Type

    text: str


GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict = Union[
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody1TypedDict,
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict,
]


GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSONResponseBody2 = (
    Union[
        GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody1,
        GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBody2,
    ]
)


GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONContentTypedDict = Union[
    str,
    List[
        GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSONResponseBody2TypedDict
    ],
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONContent = Union[
    str,
    List[
        GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSONResponseBody2
    ],
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody3Type(
    str, Enum
):
    FUNCTION = "function"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFunctionTypedDict(
    TypedDict
):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFunction(
    BaseModel
):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONToolCallsTypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody3Type
    function: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONToolCalls(
    BaseModel
):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody3Type

    function: (
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFunction
    )

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMessagesTypedDict(
    TypedDict
):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONRole
    r"""The role of the prompt message"""
    content: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONToolCallsTypedDict
        ]
    ]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMessages(
    BaseModel
):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONRole
    r"""The role of the prompt message"""

    content: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[
        List[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONToolCalls
        ]
    ] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPromptConfigTypedDict(
    TypedDict
):
    messages: List[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMessagesTypedDict
    ]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelType
    ]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONProvider
    ]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPromptConfig(
    BaseModel
):
    messages: List[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMessages
    ]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelType
    ] = None
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONProvider
    ] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMetadataTypedDict(
    TypedDict
):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMetadata(
    BaseModel
):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptTemplatesItemsVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPromptConfigTypedDict
    metadata: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMetadataTypedDict
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class GetAllPromptTemplatesItemsVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: (
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONPromptConfig
    )

    metadata: (
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONMetadata
    )

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesType(str, Enum):
    TEMPLATE = "template"


class Items3TypedDict(TypedDict):
    r"""Prompt template model returned from the API"""

    id: str
    owner: GetAllPromptTemplatesItemsOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptTemplatesItemsPromptTemplatesPromptConfigTypedDict
    metadata: GetAllPromptTemplatesItemsPromptTemplatesMetadataTypedDict
    versions: List[GetAllPromptTemplatesItemsVersionsTypedDict]
    type: GetAllPromptTemplatesItemsPromptTemplatesType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class Items3(BaseModel):
    r"""Prompt template model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptTemplatesItemsOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptTemplatesItemsPromptTemplatesPromptConfig

    metadata: GetAllPromptTemplatesItemsPromptTemplatesMetadata

    versions: List[GetAllPromptTemplatesItemsVersions]

    type: GetAllPromptTemplatesItemsPromptTemplatesType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-11-26T10:00:21.083Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesOwner2(str, Enum):
    VENDOR = "vendor"


ItemsOwnerTypedDict = Union[str, GetAllPromptTemplatesOwner2]


ItemsOwner = Union[str, GetAllPromptTemplatesOwner2]


class GetAllPromptTemplatesItemsModelType(str, Enum):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class GetAllPromptTemplatesItemsFormat(str, Enum):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesItemsQuality(str, Enum):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200Type(str, Enum):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormatPromptTemplates2TypedDict(TypedDict):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200Type


class GetAllPromptTemplatesResponseFormatPromptTemplates2(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200Type


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponseType(str, Enum):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatPromptTemplatesJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatPromptTemplatesJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormatPromptTemplates1TypedDict(TypedDict):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponseType
    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesJSONSchemaTypedDict


class GetAllPromptTemplatesResponseFormatPromptTemplates1(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponseType

    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesJSONSchema


GetAllPromptTemplatesItemsResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplates2TypedDict,
    GetAllPromptTemplatesResponseFormatPromptTemplates1TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptTemplatesItemsResponseFormat = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplates2,
    GetAllPromptTemplatesResponseFormatPromptTemplates1,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class GetAllPromptTemplatesItemsPhotoRealVersion(str, Enum):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class GetAllPromptTemplatesItemsEncodingFormat(str, Enum):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class GetAllPromptTemplatesItemsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptTemplatesItemsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptTemplatesItemsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[GetAllPromptTemplatesItemsResponseFormatTypedDict]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[GetAllPromptTemplatesItemsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[GetAllPromptTemplatesItemsEncodingFormat]
    r"""The format to return the embeddings"""


class GetAllPromptTemplatesItemsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptTemplatesItemsFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptTemplatesItemsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[GetAllPromptTemplatesItemsResponseFormat],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptTemplatesItemsPhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[GetAllPromptTemplatesItemsEncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsProvider(str, Enum):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class GetAllPromptTemplatesItemsRole(str, Enum):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesResponse200Type(str, Enum):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2PromptTemplatesImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplates2TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200Type
    image_url: GetAllPromptTemplates2PromptTemplatesImageURLTypedDict


class GetAllPromptTemplates2PromptTemplates2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200Type

    image_url: GetAllPromptTemplates2PromptTemplatesImageURL


class GetAllPromptTemplates2PromptTemplatesResponseType(str, Enum):
    TEXT = "text"


class GetAllPromptTemplates2PromptTemplates1TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponseType
    text: str


class GetAllPromptTemplates2PromptTemplates1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponseType

    text: str


GetAllPromptTemplatesContentPromptTemplates2TypedDict = Union[
    GetAllPromptTemplates2PromptTemplates1TypedDict,
    GetAllPromptTemplates2PromptTemplates2TypedDict,
]


GetAllPromptTemplatesContentPromptTemplates2 = Union[
    GetAllPromptTemplates2PromptTemplates1, GetAllPromptTemplates2PromptTemplates2
]


GetAllPromptTemplatesItemsContentTypedDict = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplates2TypedDict]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptTemplatesItemsContent = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplates2]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONType(
    str, Enum
):
    FUNCTION = "function"


class GetAllPromptTemplatesItemsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsToolCallsTypedDict(TypedDict):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONType
    function: GetAllPromptTemplatesItemsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptTemplatesItemsToolCalls(BaseModel):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONType

    function: GetAllPromptTemplatesItemsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptTemplatesItemsMessagesTypedDict(TypedDict):
    role: GetAllPromptTemplatesItemsRole
    r"""The role of the prompt message"""
    content: GetAllPromptTemplatesItemsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[GetAllPromptTemplatesItemsToolCallsTypedDict]]


class GetAllPromptTemplatesItemsMessages(BaseModel):
    role: GetAllPromptTemplatesItemsRole
    r"""The role of the prompt message"""

    content: GetAllPromptTemplatesItemsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[GetAllPromptTemplatesItemsToolCalls]] = None


class GetAllPromptTemplatesItemsPromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptTemplatesItemsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptTemplatesItemsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[GetAllPromptTemplatesItemsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptTemplatesItemsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptTemplatesItemsPromptConfig(BaseModel):
    messages: List[GetAllPromptTemplatesItemsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptTemplatesItemsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[GetAllPromptTemplatesItemsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptTemplatesItemsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptTemplatesItemsMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponseModelType(str, Enum):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Format(str, Enum):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesItemsPromptTemplatesResponseQuality(str, Enum):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type(
    str, Enum
):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2002TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2002(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItemsType(
    str, Enum
):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200JSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200JSONSchema(
    BaseModel
):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2001TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItemsType
    json_schema: (
        GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200JSONSchemaTypedDict
    )


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2001(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItemsType

    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200JSONSchema


GetAllPromptTemplatesItemsPromptTemplatesResponseResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2002TypedDict,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2001TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptTemplatesItemsPromptTemplatesResponseResponseFormat = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2002,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse2001,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class GetAllPromptTemplatesItemsPromptTemplatesResponsePhotoRealVersion(str, Enum):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class GetAllPromptTemplatesItemsPromptTemplatesResponseEncodingFormat(str, Enum):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class GetAllPromptTemplatesItemsPromptTemplatesResponseModelParametersTypedDict(
    TypedDict
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponse200Format]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponseQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponseResponseFormatTypedDict
        ]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponsePhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponseEncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptTemplatesItemsPromptTemplatesResponseModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponse200Format] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponseQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponseResponseFormat
        ],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptTemplatesItemsPromptTemplatesResponsePhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponseEncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponseProvider(str, Enum):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class GetAllPromptTemplatesItemsPromptTemplatesResponseRole(str, Enum):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type(
    str, Enum
):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2PromptTemplatesResponse200ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse200ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse2002TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type
    image_url: GetAllPromptTemplates2PromptTemplatesResponse200ImageURLTypedDict


class GetAllPromptTemplates2PromptTemplatesResponse2002(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems2Type

    image_url: GetAllPromptTemplates2PromptTemplatesResponse200ImageURL


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItemsType(
    str, Enum
):
    TEXT = "text"


class GetAllPromptTemplates2PromptTemplatesResponse2001TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItemsType
    text: str


class GetAllPromptTemplates2PromptTemplatesResponse2001(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItemsType

    text: str


GetAllPromptTemplatesContentPromptTemplatesResponse2002TypedDict = Union[
    GetAllPromptTemplates2PromptTemplatesResponse2001TypedDict,
    GetAllPromptTemplates2PromptTemplatesResponse2002TypedDict,
]


GetAllPromptTemplatesContentPromptTemplatesResponse2002 = Union[
    GetAllPromptTemplates2PromptTemplatesResponse2001,
    GetAllPromptTemplates2PromptTemplatesResponse2002,
]


GetAllPromptTemplatesItemsPromptTemplatesResponseContentTypedDict = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplatesResponse2002TypedDict]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptTemplatesItemsPromptTemplatesResponseContent = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplatesResponse2002]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyType(
    str, Enum
):
    FUNCTION = "function"


class GetAllPromptTemplatesItemsPromptTemplatesResponseFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponseFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponseToolCallsTypedDict(TypedDict):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyType
    function: GetAllPromptTemplatesItemsPromptTemplatesResponseFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptTemplatesItemsPromptTemplatesResponseToolCalls(BaseModel):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBodyType

    function: GetAllPromptTemplatesItemsPromptTemplatesResponseFunction

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponseMessagesTypedDict(TypedDict):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponseRole
    r"""The role of the prompt message"""
    content: GetAllPromptTemplatesItemsPromptTemplatesResponseContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[GetAllPromptTemplatesItemsPromptTemplatesResponseToolCallsTypedDict]
    ]


class GetAllPromptTemplatesItemsPromptTemplatesResponseMessages(BaseModel):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponseRole
    r"""The role of the prompt message"""

    content: GetAllPromptTemplatesItemsPromptTemplatesResponseContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[
        List[GetAllPromptTemplatesItemsPromptTemplatesResponseToolCalls]
    ] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponsePromptConfigTypedDict(TypedDict):
    messages: List[GetAllPromptTemplatesItemsPromptTemplatesResponseMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponseModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponseModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponseProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponsePromptConfig(BaseModel):
    messages: List[GetAllPromptTemplatesItemsPromptTemplatesResponseMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponseModelType] = (
        None
    )
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponseModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponseProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponseMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponseMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class ItemsVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: (
        GetAllPromptTemplatesItemsPromptTemplatesResponsePromptConfigTypedDict
    )
    metadata: GetAllPromptTemplatesItemsPromptTemplatesResponseMetadataTypedDict
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class ItemsVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptTemplatesItemsPromptTemplatesResponsePromptConfig

    metadata: GetAllPromptTemplatesItemsPromptTemplatesResponseMetadata

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsType(str, Enum):
    SNIPPET = "snippet"


class Items2TypedDict(TypedDict):
    r"""Prompt snippet model returned from the API"""

    id: str
    owner: ItemsOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: GetAllPromptTemplatesItemsPromptConfigTypedDict
    metadata: GetAllPromptTemplatesItemsMetadataTypedDict
    versions: List[ItemsVersionsTypedDict]
    key: str
    type: GetAllPromptTemplatesItemsType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class Items2(BaseModel):
    r"""Prompt snippet model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: ItemsOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptTemplatesItemsPromptConfig

    metadata: GetAllPromptTemplatesItemsMetadata

    versions: List[ItemsVersions]

    key: str

    type: GetAllPromptTemplatesItemsType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-11-26T10:00:21.083Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesOwnerPromptTemplatesResponse2(str, Enum):
    VENDOR = "vendor"


GetAllPromptTemplatesItemsPromptTemplatesOwnerTypedDict = Union[
    str, GetAllPromptTemplatesOwnerPromptTemplatesResponse2
]


GetAllPromptTemplatesItemsPromptTemplatesOwner = Union[
    str, GetAllPromptTemplatesOwnerPromptTemplatesResponse2
]


class ItemsModelType(str, Enum):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class ItemsFormat(str, Enum):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class ItemsQuality(str, Enum):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesType(str, Enum):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormat2TypedDict(TypedDict):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesType


class GetAllPromptTemplatesResponseFormat2(BaseModel):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesType


class GetAllPromptTemplatesResponseFormatType(str, Enum):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatJSONSchemaTypedDict(TypedDict):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatJSONSchema(BaseModel):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormat1TypedDict(TypedDict):
    type: GetAllPromptTemplatesResponseFormatType
    json_schema: GetAllPromptTemplatesResponseFormatJSONSchemaTypedDict


class GetAllPromptTemplatesResponseFormat1(BaseModel):
    type: GetAllPromptTemplatesResponseFormatType

    json_schema: GetAllPromptTemplatesResponseFormatJSONSchema


ItemsResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormat2TypedDict,
    GetAllPromptTemplatesResponseFormat1TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


ItemsResponseFormat = Union[
    GetAllPromptTemplatesResponseFormat2, GetAllPromptTemplatesResponseFormat1
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class ItemsPhotoRealVersion(str, Enum):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class ItemsEncodingFormat(str, Enum):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class ItemsModelParametersTypedDict(TypedDict):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[ItemsFormat]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[ItemsQuality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[Nullable[ItemsResponseFormatTypedDict]]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[ItemsPhotoRealVersion]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[ItemsEncodingFormat]
    r"""The format to return the embeddings"""


class ItemsModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[ItemsFormat] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[ItemsQuality] = None
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[ItemsResponseFormat], pydantic.Field(alias="responseFormat")
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[ItemsPhotoRealVersion], pydantic.Field(alias="photoRealVersion")
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[ItemsEncodingFormat] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ItemsProvider(str, Enum):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class ItemsRole(str, Enum):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesType(str, Enum):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2ImageURLTypedDict(TypedDict):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2ImageURL(BaseModel):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates22TypedDict(TypedDict):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesType
    image_url: GetAllPromptTemplates2ImageURLTypedDict


class GetAllPromptTemplates22(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesType

    image_url: GetAllPromptTemplates2ImageURL


class GetAllPromptTemplates2Type(str, Enum):
    TEXT = "text"


class GetAllPromptTemplates21TypedDict(TypedDict):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2Type
    text: str


class GetAllPromptTemplates21(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2Type

    text: str


GetAllPromptTemplatesContent2TypedDict = Union[
    GetAllPromptTemplates21TypedDict, GetAllPromptTemplates22TypedDict
]


GetAllPromptTemplatesContent2 = Union[GetAllPromptTemplates21, GetAllPromptTemplates22]


ItemsContentTypedDict = Union[str, List[GetAllPromptTemplatesContent2TypedDict]]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


ItemsContent = Union[str, List[GetAllPromptTemplatesContent2]]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponseType(str, Enum):
    FUNCTION = "function"


class ItemsFunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class ItemsFunction(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class ItemsToolCallsTypedDict(TypedDict):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponseType
    function: ItemsFunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class ItemsToolCalls(BaseModel):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponseType

    function: ItemsFunction

    id: Optional[str] = None

    index: Optional[float] = None


class ItemsMessagesTypedDict(TypedDict):
    role: ItemsRole
    r"""The role of the prompt message"""
    content: ItemsContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[List[ItemsToolCallsTypedDict]]


class ItemsMessages(BaseModel):
    role: ItemsRole
    r"""The role of the prompt message"""

    content: ItemsContent
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[List[ItemsToolCalls]] = None


class ItemsPromptConfigTypedDict(TypedDict):
    messages: List[ItemsMessagesTypedDict]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[ItemsModelType]
    r"""The type of the model"""
    model_parameters: NotRequired[ItemsModelParametersTypedDict]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[ItemsProvider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class ItemsPromptConfig(BaseModel):
    messages: List[ItemsMessages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[ItemsModelType] = None
    r"""The type of the model"""

    model_parameters: Optional[ItemsModelParameters] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[ItemsProvider] = None

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ItemsMetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class ItemsMetadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelType(str, Enum):
    r"""The type of the model"""

    CHAT = "chat"
    COMPLETION = "completion"
    EMBEDDING = "embedding"
    VISION = "vision"
    IMAGE = "image"
    TTS = "tts"
    STT = "stt"
    RERANK = "rerank"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFormat(
    str, Enum
):
    r"""Only supported on `image` models."""

    URL = "url"
    B64_JSON = "b64_json"
    TEXT = "text"
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Quality(str, Enum):
    r"""Only supported on `image` models."""

    STANDARD = "standard"
    HD = "hd"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType(
    str, Enum
):
    JSON_OBJECT = "json_object"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON2TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON2(
    BaseModel
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type(
    str, Enum
):
    JSON_SCHEMA = "json_schema"


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONJSONSchemaTypedDict(
    TypedDict
):
    name: str
    strict: bool
    schema_: Dict[str, Any]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONJSONSchema(
    BaseModel
):
    name: str

    strict: bool

    schema_: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON1TypedDict(
    TypedDict
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type
    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONJSONSchemaTypedDict


class GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON1(
    BaseModel
):
    type: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type

    json_schema: GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSONJSONSchema


GetAllPromptTemplatesItemsPromptTemplatesResponse200ResponseFormatTypedDict = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON2TypedDict,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON1TypedDict,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


GetAllPromptTemplatesItemsPromptTemplatesResponse200ResponseFormat = Union[
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON2,
    GetAllPromptTemplatesResponseFormatPromptTemplatesResponse200ApplicationJSON1,
]
r"""An object specifying the format that the model must output.

Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200PhotoRealVersion(str, Enum):
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    V1 = "v1"
    V2 = "v2"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200EncodingFormat(str, Enum):
    r"""The format to return the embeddings"""

    FLOAT = "float"
    BASE64 = "base64"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelParametersTypedDict(
    TypedDict
):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    max_tokens: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_k: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    top_p: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    frequency_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    presence_penalty: NotRequired[float]
    r"""Only supported on `chat` and `completion` models."""
    num_images: NotRequired[float]
    r"""Only supported on `image` models."""
    seed: NotRequired[float]
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""
    format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFormat
    ]
    r"""Only supported on `image` models."""
    dimensions: NotRequired[str]
    r"""Only supported on `image` models."""
    quality: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponse200Quality]
    r"""Only supported on `image` models."""
    style: NotRequired[str]
    r"""Only supported on `image` models."""
    response_format: NotRequired[
        Nullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ResponseFormatTypedDict
        ]
    ]
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """
    photo_real_version: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200PhotoRealVersion
    ]
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""
    encoding_format: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200EncodingFormat
    ]
    r"""The format to return the embeddings"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelParameters(BaseModel):
    r"""Model Parameters: Not all parameters apply to every model"""

    temperature: Optional[float] = None
    r"""Only supported on `chat` and `completion` models."""

    max_tokens: Annotated[Optional[float], pydantic.Field(alias="maxTokens")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_k: Annotated[Optional[float], pydantic.Field(alias="topK")] = None
    r"""Only supported on `chat` and `completion` models."""

    top_p: Annotated[Optional[float], pydantic.Field(alias="topP")] = None
    r"""Only supported on `chat` and `completion` models."""

    frequency_penalty: Annotated[
        Optional[float], pydantic.Field(alias="frequencyPenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    presence_penalty: Annotated[
        Optional[float], pydantic.Field(alias="presencePenalty")
    ] = None
    r"""Only supported on `chat` and `completion` models."""

    num_images: Annotated[Optional[float], pydantic.Field(alias="numImages")] = None
    r"""Only supported on `image` models."""

    seed: Optional[float] = None
    r"""Best effort deterministic seed for the model. Currently only OpenAI models support these"""

    format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONFormat
    ] = None
    r"""Only supported on `image` models."""

    dimensions: Optional[str] = None
    r"""Only supported on `image` models."""

    quality: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponse200Quality] = (
        None
    )
    r"""Only supported on `image` models."""

    style: Optional[str] = None
    r"""Only supported on `image` models."""

    response_format: Annotated[
        OptionalNullable[
            GetAllPromptTemplatesItemsPromptTemplatesResponse200ResponseFormat
        ],
        pydantic.Field(alias="responseFormat"),
    ] = UNSET
    r"""An object specifying the format that the model must output.

    Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema

    Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.

    Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    """

    photo_real_version: Annotated[
        Optional[GetAllPromptTemplatesItemsPromptTemplatesResponse200PhotoRealVersion],
        pydantic.Field(alias="photoRealVersion"),
    ] = None
    r"""The version of photoReal to use. Must be v1 or v2. Only available for `leonardoai` provider"""

    encoding_format: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200EncodingFormat
    ] = None
    r"""The format to return the embeddings"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "maxTokens",
            "topK",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "numImages",
            "seed",
            "format",
            "dimensions",
            "quality",
            "style",
            "responseFormat",
            "photoRealVersion",
            "encoding_format",
        ]
        nullable_fields = ["responseFormat"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Provider(str, Enum):
    COHERE = "cohere"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    REPLICATE = "replicate"
    GOOGLE = "google"
    GOOGLE_AI = "google-ai"
    AZURE = "azure"
    AWS = "aws"
    ANYSCALE = "anyscale"
    PERPLEXITY = "perplexity"
    GROQ = "groq"
    FAL = "fal"
    LEONARDOAI = "leonardoai"
    NVIDIA = "nvidia"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Role(str, Enum):
    r"""The role of the prompt message"""

    SYSTEM = "system"
    ASSISTANT = "assistant"
    USER = "user"
    EXCEPTION = "exception"
    TOOL = "tool"
    PROMPT = "prompt"
    CORRECTION = "correction"
    EXPECTED_OUTPUT = "expected_output"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType(
    str, Enum
):
    IMAGE_URL = "image_url"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONImageURLTypedDict(
    TypedDict
):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""
    id: NotRequired[str]
    r"""The orq.ai id of the image"""
    detail: NotRequired[str]
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONImageURL(
    BaseModel
):
    url: str
    r"""Either a URL of the image or the base64 encoded data URI."""

    id: Optional[str] = None
    r"""The orq.ai id of the image"""

    detail: Optional[str] = None
    r"""Specifies the detail level of the image. Currently only supported with OpenAI models"""


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON2TypedDict(
    TypedDict
):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType
    image_url: (
        GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONImageURLTypedDict
    )


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON2(BaseModel):
    r"""The image part of the prompt message. Only supported with vision models."""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1VersionsType

    image_url: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONImageURL


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type(
    str, Enum
):
    TEXT = "text"


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON1TypedDict(
    TypedDict
):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type
    text: str


class GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON1(BaseModel):
    r"""Text content part of a prompt message"""

    type: GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSONResponseBodyItems1Type

    text: str


GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSON2TypedDict = Union[
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON1TypedDict,
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON2TypedDict,
]


GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSON2 = Union[
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON1,
    GetAllPromptTemplates2PromptTemplatesResponse200ApplicationJSON2,
]


GetAllPromptTemplatesItemsPromptTemplatesResponse200ContentTypedDict = Union[
    str,
    List[
        GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSON2TypedDict
    ],
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


GetAllPromptTemplatesItemsPromptTemplatesResponse200Content = Union[
    str, List[GetAllPromptTemplatesContentPromptTemplatesResponse200ApplicationJSON2]
]
r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody1Type(
    str, Enum
):
    FUNCTION = "function"


class GetAllPromptTemplatesItemsPromptTemplatesResponse200FunctionTypedDict(TypedDict):
    name: str
    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Function(BaseModel):
    name: str

    arguments: str
    r"""JSON string arguments for the functions"""


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ToolCallsTypedDict(TypedDict):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody1Type
    function: GetAllPromptTemplatesItemsPromptTemplatesResponse200FunctionTypedDict
    id: NotRequired[str]
    index: NotRequired[float]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200ToolCalls(BaseModel):
    type: GetAllPromptTemplatesItemsPromptTemplatesResponse200ApplicationJSONResponseBody1Type

    function: GetAllPromptTemplatesItemsPromptTemplatesResponse200Function

    id: Optional[str] = None

    index: Optional[float] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200MessagesTypedDict(TypedDict):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponse200Role
    r"""The role of the prompt message"""
    content: GetAllPromptTemplatesItemsPromptTemplatesResponse200ContentTypedDict
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""
    tool_calls: NotRequired[
        List[GetAllPromptTemplatesItemsPromptTemplatesResponse200ToolCallsTypedDict]
    ]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Messages(BaseModel):
    role: GetAllPromptTemplatesItemsPromptTemplatesResponse200Role
    r"""The role of the prompt message"""

    content: GetAllPromptTemplatesItemsPromptTemplatesResponse200Content
    r"""The contents of the user message. Either the text content of the message or an array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts."""

    tool_calls: Optional[
        List[GetAllPromptTemplatesItemsPromptTemplatesResponse200ToolCalls]
    ] = None


class GetAllPromptTemplatesItemsPromptTemplatesResponse200PromptConfigTypedDict(
    TypedDict
):
    messages: List[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200MessagesTypedDict
    ]
    stream: NotRequired[bool]
    model: NotRequired[str]
    model_db_id: NotRequired[str]
    r"""The id of the resource"""
    model_type: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelType
    ]
    r"""The type of the model"""
    model_parameters: NotRequired[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelParametersTypedDict
    ]
    r"""Model Parameters: Not all parameters apply to every model"""
    provider: NotRequired[GetAllPromptTemplatesItemsPromptTemplatesResponse200Provider]
    integration_id: NotRequired[Nullable[str]]
    r"""The id of the resource"""
    version: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200PromptConfig(BaseModel):
    messages: List[GetAllPromptTemplatesItemsPromptTemplatesResponse200Messages]

    stream: Optional[bool] = None

    model: Optional[str] = None

    model_db_id: Optional[str] = None
    r"""The id of the resource"""

    model_type: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelType
    ] = None
    r"""The type of the model"""

    model_parameters: Optional[
        GetAllPromptTemplatesItemsPromptTemplatesResponse200ModelParameters
    ] = None
    r"""Model Parameters: Not all parameters apply to every model"""

    provider: Optional[GetAllPromptTemplatesItemsPromptTemplatesResponse200Provider] = (
        None
    )

    integration_id: OptionalNullable[str] = UNSET
    r"""The id of the resource"""

    version: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "model",
            "model_db_id",
            "model_type",
            "model_parameters",
            "provider",
            "integration_id",
            "version",
        ]
        nullable_fields = ["integration_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class GetAllPromptTemplatesItemsPromptTemplatesResponse200MetadataTypedDict(TypedDict):
    use_cases: NotRequired[List[str]]
    language: NotRequired[str]


class GetAllPromptTemplatesItemsPromptTemplatesResponse200Metadata(BaseModel):
    use_cases: Optional[List[str]] = None

    language: Optional[str] = None


class GetAllPromptTemplatesItemsPromptTemplatesVersionsTypedDict(TypedDict):
    r"""Prompt version model returned from the API"""

    id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: (
        GetAllPromptTemplatesItemsPromptTemplatesResponse200PromptConfigTypedDict
    )
    metadata: GetAllPromptTemplatesItemsPromptTemplatesResponse200MetadataTypedDict
    commit: str
    timestamp: str
    description: NotRequired[Nullable[str]]


class GetAllPromptTemplatesItemsPromptTemplatesVersions(BaseModel):
    r"""Prompt version model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: GetAllPromptTemplatesItemsPromptTemplatesResponse200PromptConfig

    metadata: GetAllPromptTemplatesItemsPromptTemplatesResponse200Metadata

    commit: str

    timestamp: str

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ItemsType(str, Enum):
    PROMPT = "prompt"


class Items1TypedDict(TypedDict):
    r"""Prompt model returned from the API"""

    id: str
    owner: GetAllPromptTemplatesItemsPromptTemplatesOwnerTypedDict
    domain_id: str
    created_by_id: str
    display_name: str
    updated_by_id: str
    prompt_config: ItemsPromptConfigTypedDict
    metadata: ItemsMetadataTypedDict
    versions: List[GetAllPromptTemplatesItemsPromptTemplatesVersionsTypedDict]
    type: ItemsType
    description: NotRequired[Nullable[str]]
    created: NotRequired[datetime]
    r"""The date and time the resource was created"""
    updated: NotRequired[datetime]
    r"""The date and time the resource was last updated"""


class Items1(BaseModel):
    r"""Prompt model returned from the API"""

    id: Annotated[str, pydantic.Field(alias="_id")]

    owner: GetAllPromptTemplatesItemsPromptTemplatesOwner

    domain_id: str

    created_by_id: str

    display_name: str

    updated_by_id: str

    prompt_config: ItemsPromptConfig

    metadata: ItemsMetadata

    versions: List[GetAllPromptTemplatesItemsPromptTemplatesVersions]

    type: ItemsType

    description: OptionalNullable[str] = UNSET

    created: Optional[datetime] = None
    r"""The date and time the resource was created"""

    updated: Optional[datetime] = dateutil.parser.isoparse("2024-11-26T10:00:21.083Z")
    r"""The date and time the resource was last updated"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "created", "updated"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


ItemsTypedDict = Union[Items1TypedDict, Items3TypedDict, Items2TypedDict]


Items = Union[Items1, Items3, Items2]


class GetAllPromptTemplatesResponseBodyTypedDict(TypedDict):
    r"""Prompt templates retrieved."""

    count: float
    items: List[ItemsTypedDict]


class GetAllPromptTemplatesResponseBody(BaseModel):
    r"""Prompt templates retrieved."""

    count: float

    items: List[Items]
