from __future__ import annotations

import collections
from collections.abc import Sequence

import numpy as np
from scipy.spatial.distance import jensenshannon

from qek.data.dataset import ProcessedData


class QuantumEvolutionKernel:
    def __init__(self, mu: float):
        self.mu = mu

    def __call__(
        self, graph_1: ProcessedData, graph_2: ProcessedData, size_max: int = 100
    ) -> float:
        """Compute the similarity between two graphs using Jensen-Shannon divergence.

        This method computes the square of the Jensen-Shannon divergence (JSD)
        between two probability distributions over bitstrings. The JSD is a
        measure of the difference between two probability distributions, and it
        can be used as a kernel for machine learning algorithms that require a
        similarity function.

        The input graphs are assumed to have been processed using the ProcessedData
        class from qek_os.data_io.dataset. The size_max parameter controls the maximum
        length of the bitstrings considered in the computation.
        Args:
            graph_1 (ProcessedData): First graph.
            graph_2 (ProcessedData): Second graph.
            size_max (float, optional): Maximum length of bitstrings to consider. Defaults to -1.

        Returns:
            float: Similarity between the two graphs, scaled by a factor that depends on mu.

        Notes:
            The JSD is computed using the jensenshannon function from scipy.spatial.distance,
            and it is squared because jensenshannon scipy function output the distance instead
            of the divergence.
        """
        dist_graph_1 = dist_excitation_and_vec(
            count_bitstring=graph_1.state_dict, size_max=size_max
        )
        dist_graph_2 = dist_excitation_and_vec(
            count_bitstring=graph_2.state_dict, size_max=size_max
        )
        js = (
            jensenshannon(p=dist_graph_1, q=dist_graph_2) ** 2
        )  # Because the divergence is the square root of the distance
        return float(np.exp(-self.mu * js))

    def create_train_kernel_matrix(self, train_dataset: Sequence[ProcessedData]) -> np.ndarray:
        """Compute a kernel matrix for a given training dataset.

        This method computes a symmetric N x N kernel matrix from the Jensen-Shannon
        divergences between all pairs of graphs in the input dataset. The resulting matrix
        can be used as a similarity metric for machine learning algorithms.
        Args:
            train_dataset (Sequence[ProcessedData]): A list of ProcessedData objects to compute
            the kernel matrix from.
        Returns:
            np.ndarray: An N x N symmetric matrix where the entry at row i and column j represents
            the similarity between the graphs in positions i and j of the input dataset.
        """
        N = len(train_dataset)
        kernel_mat = np.zeros((N, N))
        for i in range(N):
            for j in range(i + 1, N):
                kernel_mat[i][j] = self(train_dataset[i], train_dataset[j])
                kernel_mat[j][i] = kernel_mat[i][j]
        return kernel_mat

    def create_test_kernel_matrix(
        self,
        test_dataset: Sequence[ProcessedData],
        train_dataset: Sequence[ProcessedData],
    ) -> np.ndarray:
        """Compute a kernel matrix for a given testing dataset and training set.

        This method computes an N x M kernel matrix from the Jensen-Shannon
        divergences between all pairs of graphs in the input testing dataset
        and the training dataset.
        The resulting matrix can be used as a similarity metric for machine learning algorithms,
        particularly when evaluating the performance on the test dataset using a trained model.
        Args:
            test_dataset (Sequence[ProcessedData]): A list of ProcessedData
            objects representing the testing dataset.
            train_dataset (Sequence[ProcessedData]): A list of ProcessedData
            objects representing the training set.
        Returns:
            np.ndarray: An M x N matrix where the entry at row i and column j represents
            the similarity between the graph in position i of the test dataset
            and the graph in position j of the training set.
        """
        N_train = len(train_dataset)
        N_test = len(test_dataset)
        kernel_mat = np.zeros((N_test, N_train))
        for i in range(N_test):
            for j in range(N_train):
                kernel_mat[i][j] = self(test_dataset[i], train_dataset[j])
        return kernel_mat


def count_occupation_from_bitstring(bitstring: str) -> int:
    """Counts the number of '1' bits in a binary string.

    Args:
        bitstring (str): A binary string containing only '0's and '1's.

    Returns:
        int: The number of '1' bits found in the input string.
    """
    return sum(int(bit) for bit in bitstring)


def dist_excitation_and_vec(count_bitstring: dict[str, int], size_max: int) -> np.ndarray:
    """Calculates the distribution of excitation energies from a dictionary of
    bitstrings to their respective counts, and then creates a NumPy vector with the
    results.

    Args:
        count_bitstring (dict[str, int]): A dictionary mapping binary strings
            to their counts.
        size_max (int): The maximum size of the resulting NumPy array.

    Returns:
        np.ndarray: A NumPy array where keys are the number of '1' bits
            in each binary string and values are the normalized counts.
    """
    count_occ: dict = collections.defaultdict(float)
    total = 0.0
    for k, v in count_bitstring.items():
        nbr_occ = count_occupation_from_bitstring(k)
        count_occ[nbr_occ] += v
        total += v

    numpy_vec = np.zeros(size_max)
    for k, v in count_occ.items():
        if int(k) <= size_max:
            numpy_vec[k] = v / total

    return numpy_vec
