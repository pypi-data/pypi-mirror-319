invoke>=2.0.0
onnx>=1.11.0
onnxmltools==1.10.0
torch>=1.12.1
pyyaml>=5.4
typeguard>=2.3.13
packaging>=20.9
numpy<2.0.0
pandas>=1.5.3
fasteners
GitPython>=3.1.40
psutil
wmi
pytz

[:extra == "llm-oga-cuda"]
onnxruntime-gpu>=1.19.1

[:platform_system == "Linux" and extra != "llm-oga-cuda"]
onnxruntime>=1.10.1

[:platform_system == "Windows" and extra != "llm-oga-cuda"]
onnxruntime-directml>=1.19.0

[llm]
tqdm
torch>=2.0.0
transformers
accelerate
py-cpuinfo
sentencepiece
datasets
fastapi
uvicorn[standard]

[llm-oga-cuda]
onnxruntime-genai-cuda==0.4.0
tqdm
torch<2.4,>=2.0.0
transformers<4.45.0
accelerate
py-cpuinfo
sentencepiece
datasets
fastapi
uvicorn[standard]

[llm-oga-dml]
onnxruntime-genai-directml==0.4.0
tqdm
torch<2.4,>=2.0.0
transformers<4.45.0
accelerate
py-cpuinfo
sentencepiece
datasets
fastapi
uvicorn[standard]

[llm-oga-hybrid]
transformers
torch
onnx==1.16.1
numpy==1.26.4
datasets
fastapi
uvicorn[standard]

[llm-oga-npu]
transformers
torch
onnx==1.16.0
onnxruntime==1.18.0
numpy==1.26.4
tqdm
accelerate
py-cpuinfo
sentencepiece
datasets
fastapi
uvicorn[standard]
