{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d169ce5-e121-48c4-a97d-06d6ce1a03b8",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "This notebooks will present the tokenization of sequence for prokbert.\n",
    "Parts are:\n",
    "  * Tokenization process background and paramters\n",
    "  * Tokenization of sequences\n",
    "  * Tokenization for pretraining\n",
    "  * HDF datasets for storing preprocessed sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d384f0-2b14-48da-a9a9-ed7d76b427c0",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization of Sequence Data\n",
    "\n",
    "ProkBERT employs LCA tokenization, leveraging overlapping k-mers to capture rich local context information, enhancing model generalization and performance. The key parameters are the k-mer size and shift. For instance, with a k-mer size of 6 and a shift of 1, the tokenization captures detailed sequence information, while a k-mer size of 1 represents a basic character-based approach.\n",
    "\n",
    "### Segmentation Strategies\n",
    "Before tokenization, sequences are segmented using two main approaches:\n",
    "1. **Contiguous Sampling**: Divides contigs into non-overlapping segments.\n",
    "2. **Random Sampling**: Fragments the input sequence into randomly sized segments.\n",
    "\n",
    "### Tokenization Process\n",
    "After segmentation, sequences are encoded into a simpler vector format. The LCA method is pivotal in this phase, allowing the model to use a broader context and reducing computational demands while maintaining the information-rich local context.\n",
    "\n",
    "### Context Size Limitations\n",
    "It's important to note that transformer models, including ProkBERT, have a context size limitation. ProkBERT's design accommodates context sizes significantly larger than an average gene, yet smaller than the average bacterial genome.\n",
    "\n",
    "We provide pretrained models for variants like ProkBERT-mini (k-mer size 6, shift 1), ProkBERT-mini-c (k-mer size 1, shift 1), and ProkBERT-mini-long (k-mer size 6, shift 2), catering to different sequence analysis requirements.\n",
    "\n",
    "<img src=\"https://github.com/nbrg-ppcu/prokbert/blob/main/assets/Figure2_tokenization.png?raw=true\" width=\"800\" alt=\"Segmentation Process\"> \n",
    "\n",
    "*Figure: The tokenization process in ProkBERT.*\n",
    "\n",
    "It is important to see that, when the $shift > 1$ there are multiple possible tokenization, depending where we start the tokenizaiton. The window offset refers the actual tokenization window. \n",
    "I.e. let the sequence be `ATGTCCGCGACCTTTCATACATACCACCGGTAC` with the k-mer size 6, shift 2) we will have two possible tokenization:\n",
    "\n",
    "Tokenization with offset=0:\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.  ATGTCC  GACCTT  ATACAT  CACCGG\n",
    "1.    GTCCGC  CCTTTC  ACATAC  CCGGTA\n",
    "2.      CCGCGA  TTTCAT  ATACCA\n",
    "3.        GCGACC  TCATAC  ACCACC\n",
    "```\n",
    "Tokenization with offset=1\n",
    "```plaintext\n",
    "    ATGTCCGCGACCTTTCATACATACCACCGGTAC\n",
    "0.   TGTCCG  ACCTTT  TACATA  ACCGGT\n",
    "1.     TCCGCG  CTTTCA  CATACC  CGGTAC\n",
    "2.       CGCGAC  TTCATA  TACCAC\n",
    "3.         CGACCT  CATACA  CCACCG\n",
    "```\n",
    "By default all possible tokenization is returned. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c345ea8-d8a0-4d57-90c9-7d36bc9dabd9",
   "metadata": {},
   "source": [
    "## Tokenization parameters\n",
    "\n",
    "The following table outlines the configuration parameters for ProkBERT, detailing their purpose, default values, types, and constraints.\n",
    "\n",
    "\n",
    "| Parameter | Description | Type | Default | Constraints |\n",
    "|-----------|-------------|------|---------|-------------|\n",
    "| **Tokenization** |\n",
    "| `type` | Describes the tokenization approach. By default, the LCA (Local Context Aware) method is used. | string | `lca` | Options: `lca` |\n",
    "| `kmer` | Determines the k-mer size for the tokenization process. | integer | 6 | Options: 1-9 |\n",
    "| `shift` | Represents the shift parameter in k-mer. The default value is 1. | integer | 1 | Min: 0 |\n",
    "| `max_segment_length` | Gives the maximum number of characters in a segment. This should be consistent with the language model's capability. It can be alternated with token_limit. | integer | 2050 | Min: 6, Max: 4294967296 |\n",
    "| `token_limit` | States the maximum token count that the language model can process, inclusive of special tokens like CLS and SEP. This is interchangeable with max_segment_length. | integer | 4096 | Min: 1, Max: 4294967296 |\n",
    "| `max_unknown_token_proportion` | Defines the maximum allowed proportion of unknown tokens in a sequence. For instance, if 10% of the tokens are unknown (when max_unknown_token_proportion=0.1), the segment won't be tokenized. | float | 0.9999 | Min: 0, Max: 1 |\n",
    "| `vocabfile` | Path to the vocabulary file. If set to 'auto', the default vocabulary is utilized. | str | `auto` | - |\n",
    "| `vocabmap` | The default vocabmap loaded from file | dict | `{}` | - |\n",
    "| `isPaddingToMaxLength` | Determines if the tokenized sentence should be padded with [PAD] tokens to produce vectors of a fixed length. | bool | False | Options: True, False |\n",
    "| `add_special_token` | The tokenizer should add the special starting and sentence end tokens. The default is yes. | bool | True | Options: True, False |\n",
    "| **Computation** |\n",
    "| `cpu_cores_for_segmentation` | Specifies the number of CPU cores allocated for the segmentation process. | integer | 10 | Min: 1 |\n",
    "| `cpu_cores_for_tokenization` | Allocates a certain number of CPU cores for the k-mer tokenization process. | integer | -1 | Min: 1 |\n",
    "| `batch_size_tokenization` | Determines the number of segments a single core processes at a time. The input segment list will be divided into chunks of this size. | integer | 10000 | Min: 1 |\n",
    "| `batch_size_fasta_segmentation` | Sets the number of fasta files processed in a single batch, useful when dealing with a large number of fasta files. | integer | 3 | Min: 1 |\n",
    "| `numpy_token_integer_prec_byte` | The type of integer to be used during the vectorization. The default is 2, if you want to work larger k-mers then increase it to 4. 1: np.int8, 2:np.int16. 4:np.int32. 8: np.int64 | integer | 2 | Options: 1, 2, 4, 8 |\n",
    "| `np_tokentype` | Dummy | type | `np.int16` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576a136-a8e7-4d4c-af14-2264b8995e6f",
   "metadata": {},
   "source": [
    "## Installation of ProkBERT (if needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66218845-48e1-4e3e-af3c-b4e31411e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProkBERT is already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import prokbert\n",
    "    print(\"ProkBERT is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install prokbert\n",
    "    print(\"Installed ProkBERT.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488de6fa-248a-47fa-be82-f4597c62951e",
   "metadata": {},
   "source": [
    "# Tokenization of sequences examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e72e9bf-f782-4cc2-8fc3-efefab5e2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 10:36:37,777 - INFO - Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-11-13 10:36:37,778 - INFO - NumExpr defaulting to 8 threads.\n",
      "2023-11-13 10:36:38,036 - INFO - Nr. line to cover the seq:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 954, 2910, 1437, 2442, 3] [2, 3803, 3435, 1638, 1564, 3]\n",
      "ATGTCC GTCCGC CCGCGA GCGACC\n",
      "    ATGTCCGCGACCT\n",
      "0.  ATGTCC\n",
      "1.    GTCCGC\n",
      "2.      CCGCGA\n",
      "3.        GCGACC\n"
     ]
    }
   ],
   "source": [
    "from prokbert.config_utils import *\n",
    "from prokbert.sequtils import *\n",
    "tokenization_parameters = {'kmer' : 6,\n",
    "                          'shift' : 2}\n",
    "\n",
    "segment = 'ATGTCCGCGACCT'\n",
    "defconfig = SeqConfig() # For the detailed configarion parameters see the table above\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)\n",
    "\n",
    "print(' '.join([str(t) for t in tokens]))\n",
    "print(' '.join(kmers[0]))\n",
    "\n",
    "results_pretty_print = pretty_print_overlapping_sequence(segment, kmers[0], tokenization_params)\n",
    "print(results_pretty_print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f35c62-1a37-474c-a7df-2508cccc7350",
   "metadata": {},
   "source": [
    "## Tokenizing Longer Sequences\n",
    "\n",
    "To tokenize sequences longer than what the current ProkBERT model supports, you can adjust the `token_limit` and `max_segment_length` parameters. Keep in mind that the tokenization process is parallelized using Python's multiprocessing module at the segment level. Therefore, it's important to also consider adjusting the number of cores utilized, as well as the `batch_size_tokenization` parameter, which determines how many sequences a core should process at once. Failing to appropriately adjust these settings might lead to memory issues.\n",
    "\n",
    "\n",
    "### Example Python Code for Long Sequence Tokenization\n",
    "\n",
    "Below is an example of how you can configure and use the tokenization for longer sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fdb064-3e15-4422-bc86-fe8424a11c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prokbert.sequtils import lca_tokenize_segment\n",
    "from prokbert.config_utils import SeqConfig\n",
    "\n",
    "# Tokenization parameters\n",
    "tokenization_parameters = {\n",
    "    'kmer': 6,\n",
    "    'shift': 1,\n",
    "    'max_segment_length': 2000000,\n",
    "    'token_limit': 2000000\n",
    "}\n",
    "\n",
    "# Example of a long sequence\n",
    "segment = 'ATGTCCGCGACCT' * 100000\n",
    "\n",
    "# Default configuration for tokenization\n",
    "defconfig = SeqConfig() # For detailed configuration parameters, refer to the table above\n",
    "\n",
    "# Get and set tokenization parameters\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "\n",
    "# Perform tokenization\n",
    "tokens, kmers = lca_tokenize_segment(segment, tokenization_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3a888-53db-4188-b4de-16e09752fce2",
   "metadata": {},
   "source": [
    "# Tokenization for Pretraining\n",
    "\n",
    "Given the abundance and large size of sequence data, preprocessing this data in advance and storing it for later use is a recommended practice. The primary steps in this process are segmentation and tokenization.\n",
    "\n",
    "The outcome of tokenization is a set of vectors, which need to be converted into a matrix-like structure, typically through padding. Additionally, randomizing these vectors is essential for effective training. The `sequtils` module in ProkBERT includes utilities to facilitate these steps. Below, we outline some examples of how to accomplish this.\n",
    "\n",
    "## Basic Steps for Preprocessing:\n",
    "\n",
    "1. **Load Fasta Files**: Begin by loading the raw sequence data from FASTA files.\n",
    "2. **Segment the Raw Sequences**: Apply segmentation parameters to split the sequences into manageable segments.\n",
    "3. **Tokenize the Segmented Database**: Use the defined tokenization parameters to convert the segments into tokenized forms.\n",
    "4. **Create a Padded/Truncated Array**: Generate a uniform array structure, padding or truncating as necessary.\n",
    "5. **Save the Array to HDF**: Store the processed data in an HDF (Hierarchical Data Format) file for efficient retrieval and use in training models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b918af92-82a0-4072-9437-4096a316aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1114374/1398869558.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "2023-11-13 10:36:38,613 - INFO - Loading sequence data into memory!\n",
      "2023-11-13 10:36:38,630 - INFO - Checking input DataFrame!\n",
      "2023-11-13 10:36:38,631 - INFO - Checking input sequence_id is valid primary key in the DataFrame\n",
      "2023-11-13 10:36:38,634 - INFO - Sampling 904 segments from 66 sequences.\n",
      "2023-11-13 10:36:38,883 - INFO - Doing randomization!\n",
      "2023-11-13 10:36:38,886 - INFO - Tuncating all zeros column\n",
      "2023-11-13 10:36:38,887 - INFO - Existing HDF5 file /tmp/pretraining.h5 removed successfully.\n",
      "2023-11-13 10:36:38,892 - INFO - Numpy array saved to /tmp/pretraining.h5 successfully.\n",
      "2023-11-13 10:36:38,893 - INFO - Adding database into the HDF5 file!\n",
      "2023-11-13 10:36:38,893 - INFO - Number of chunks: 1\n",
      "2023-11-13 10:36:38,895 - INFO - Writing database chunk 0 into /tmp/pretraining.h5\n",
      "2023-11-13 10:36:39,130 - INFO - Database addition finished!\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from os.path import join\n",
    "from prokbert.sequtils import *\n",
    "\n",
    "# Directory for pretraining FASTA files\n",
    "pretraining_fasta_files_dir = pkg_resources.resource_filename('prokbert','data/pretraining')\n",
    "\n",
    "# Define segmentation and tokenization parameters\n",
    "segmentation_params = {\n",
    "    'max_length': 256,  # Split the sequence into segments of length L\n",
    "    'min_length': 6,\n",
    "    'type': 'random'\n",
    "}\n",
    "tokenization_parameters = {\n",
    "    'kmer': 6,\n",
    "    'shift': 1,\n",
    "    'max_segment_length': 2003,\n",
    "    'token_limit': 2000\n",
    "}\n",
    "\n",
    "# Setup configuration\n",
    "defconfig = SeqConfig()\n",
    "segmentation_params = defconfig.get_and_set_segmentation_parameters(segmentation_params)\n",
    "tokenization_params = defconfig.get_and_set_tokenization_parameters(tokenization_parameters)\n",
    "\n",
    "# Load and segment sequences\n",
    "input_fasta_files = [join(pretraining_fasta_files_dir, file) for file in get_non_empty_files(pretraining_fasta_files_dir)]\n",
    "sequences = load_contigs(input_fasta_files, IsAddHeader=True, adding_reverse_complement=True, AsDataFrame=True, to_uppercase=True, is_add_sequence_id=True)\n",
    "segment_db = segment_sequences(sequences, segmentation_params, AsDataFrame=True)\n",
    "\n",
    "# Tokenization\n",
    "tokenized = batch_tokenize_segments_with_ids(segment_db, tokenization_params)\n",
    "expected_max_token = max(len(arr) for arrays in tokenized.values() for arr in arrays)\n",
    "X, torchdb = get_rectangular_array_from_tokenized_dataset(tokenized, tokenization_params['shift'], expected_max_token)\n",
    "\n",
    "# Save to HDF file\n",
    "hdf_file = '/tmp/pretraining.h5'\n",
    "save_to_hdf(X, hdf_file, database=torchdb, compression=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07efc-f14e-4164-bf50-3ba036d2065d",
   "metadata": {},
   "source": [
    "### Tokenization with tokenizer class\n",
    "\n",
    "The tokenizer class can be used for tokenization as well. There are various additional features as well. The tokenizer might operate on the original sequence space or k-mer space, the default is the latter. \n",
    "For example how to encode and decode sequence is important and we will give you examples here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f15ddb-c408-4531-9f31-3641f9b48414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c4990-95cc-4ef5-a7b9-3c443275b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a9682-bdf1-4471-a732-8c1eb2ece17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1555776-593c-47ba-8463-b25e2dc7b371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff931c-a184-449b-bd60-02ac2cce77ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a14cba-c7b2-470b-9f0f-4e544c588387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e600fb-7663-4ed6-b64f-a7fc9b257748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8ab92-9690-4c75-a23a-cae9adb02689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabea51-7336-43fd-8aee-e52c4b557ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228a9fc-b2f6-4820-8176-b8f8b229449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262cbc3-f3eb-450a-a7e7-9fc24e883f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
