{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b700d0-ff0e-4940-a2cc-8fbfe9873660",
   "metadata": {},
   "source": [
    "# Inference and Evaluation with the Finetuned Models\n",
    "\n",
    "In this notebook, we demonstrate how one can evaluate various finetuned models on both the promoter and phage test datasets.\n",
    "\n",
    "The main steps are:\n",
    "  * Preparing the models and datasets\n",
    "  * Setting up the parameters for the evaluation\n",
    "  * Running inference and collecting the results for each dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77070a04-3f9c-4183-bbbf-832407b376e5",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "While ProkBERT can operate on CPUs, leveraging GPUs significantly accelerates the process. Google Colab offers free GPU usage (subject to time and memory limits), making it an ideal platform for trying and experimenting with ProkBERT models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffda82f-ddb0-4107-8ecb-0d9d2cd4bfc8",
   "metadata": {},
   "source": [
    "### Enabling and testing the GPU (if you are using google colab)\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa017b-a043-4e54-8e0e-33638192f137",
   "metadata": {},
   "source": [
    "### Setting up the packages and the installs\n",
    "First, we'll install the ProkBERT package directly from its GitHub repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fb310-003a-4f05-be02-b567fd0b62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProkBERT\n",
    "!pip install git+https://github.com/nbrg-ppcu/prokbert\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from prokbert.models import BertForBinaryClassificationWithPooling\n",
    "from prokbert.prok_datasets import ProkBERTTrainingDatasetPT\n",
    "from prokbert.config_utils import ProkBERTConfig\n",
    "from prokbert.prokbert_tokenizer import ProkBERTTokenizer\n",
    "from prokbert.training_utils import compute_metrics_eval_prediction, get_torch_data_from_segmentdb_classification, \\\n",
    "evaluate_binary_classification_bert_build_pred_results, evaluate_binary_classification_bert\n",
    "from os.path import join\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cba33b-d853-4eb6-86c0-b9beeb28598e",
   "metadata": {},
   "source": [
    "Next, we'll confirm that we can connect to the GPU with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfa647-993f-4c57-9dcc-38115b962ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check if CUDA (GPU support) is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'Found GPU at: {device_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea0dc2-73ea-414e-a1d7-06a14ae3abbf",
   "metadata": {},
   "source": [
    "## Loading the Finetuned Model for Promoter Identification\n",
    "\n",
    "Next, we will download the finetuned model for promoter identification. Our binary classification model utilizes the base model enhanced with a pooling layer, which integrates information across all nucleotides in the sequence. This approach leads to better performance compared to traditional Hugging Face sentence classification models, which only consider the embedding of the special starting token [CLS].\n",
    "\n",
    "In addition, we create the corresponding tokenizer with appropriate parameters. Here, we use the 'mini' model that uses a kmer of 6 and a shift of 1. Finetuned models such as 'mini-c' and 'mini-long' are also available; if you wish to try them, adjust the tokenizer parameters accordingly.\n",
    "\n",
    "Then, we move the model to the GPU, if available, and set the model to 'evaluation' mode (we only run forward passes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb747085-7895-466f-bdd6-d2f010bd58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finetuned_model = \"neuralbioinfo/prokbert-mini-promoter\"\n",
    "kmer = 6\n",
    "shift= 1\n",
    "\n",
    "tok_params = {'kmer' : kmer,\n",
    "             'shift' : shift}\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tok_params)\n",
    "model = BertForBinaryClassificationWithPooling.from_pretrained(finetuned_model)\n",
    "\n",
    "# Get the device. \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "_=model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a53937-5ddc-4b90-80d9-0839b4ea7ff1",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "In this section, we will evaluate the test sets of the promoter dataset. We have two different types of tests: a sigma70 test (known E. coli promoters) referred to as the 'test_sigma70' set, and a multispecies dataset, which consists of promoters from various species as well as CDS sequences (non-promoters) and randomly generated sequences. For a more detailed description, see: [Bacterial Promoters Dataset](https://huggingface.co/datasets/neuralbioinfo/bacterial_promoters)\n",
    "\n",
    "Here, we convert the Hugging Face datasets into pandas dataframes. If there is no ground truth label available, please add a pseudo column with pseudo labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98f2dd-48b0-4daf-96b2-a8d9f6eca397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the predefined dataset\n",
    "dataset = load_dataset(\"neuralbioinfo/bacterial_promoters\")\n",
    "\n",
    "test_sigma70_set = dataset[\"test_sigma70\"]\n",
    "multispecies_set = dataset[\"test_multispecies\"]\n",
    "\n",
    "test_sigma70_db = test_sigma70_set.to_pandas()\n",
    "test_ms_db = multispecies_set.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89670b-83c1-4f3b-9926-adb076d0d875",
   "metadata": {},
   "source": [
    "### Tokenization and PyTorch Dataset Creation\n",
    "\n",
    "Now that we have the tokenizer, the dataset can be prepared for evaluation. Here, we prepare the data for both sigma70 and multispecies datasets.\n",
    "\n",
    "#### Creating Datasets\n",
    "\n",
    "We will process the data into a format suitable for our PyTorch model. This involves tokenizing the sequences and converting them into a format that our model can understand. The following code will convert our `test_ms_db` (multispecies dataset) and `test_sigma70_db` (sigma70 dataset) into PyTorch datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f850bd-96d4-4f95-aae1-4b23261bc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating datasets!\n",
    "[X_test, y_test, torchdb_test] = get_torch_data_from_segmentdb_classification(tokenizer, test_ms_db)\n",
    "print(f'Processing validation database!')\n",
    "[X_val, y_val, torchdb_val] = get_torch_data_from_segmentdb_classification(tokenizer, test_sigma70_db)\n",
    "\n",
    "test_ds = ProkBERTTrainingDatasetPT(X_test, y_test, AddAttentionMask=True)\n",
    "val_ds = ProkBERTTrainingDatasetPT(X_val, y_val, AddAttentionMask=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d168a1-a02d-4b8a-849d-02009fdb534d",
   "metadata": {},
   "source": [
    "## Performing Inference and Evaluation\n",
    "\n",
    "Now that both the model and the data are prepared, we will perform inference and process the dataset.\n",
    "\n",
    "For simplicity, this demonstration will be conducted on a single GPU. An important parameter to consider is the batch size, which is set to 1024 in our example. Adjust this value according to the capabilities of your GPU to ensure efficient processing.\n",
    "\n",
    "In real-world scenarios, especially when evaluating large datasets (exceeding 1,000,000 samples), we recommend utilizing Torch Distributed Data Parallel (DDP) and compiled models for optimized performance. \n",
    "\n",
    "The prediction results will be aggregated into a list of numpy arrays, each containing the predicted label, ground truth label, and logits for each class (promoter vs. non-promoter). The evaluation metrics, including AUC, Matthews Correlation Coefficient (MCC), accuracy, and others, will be summarized and returned in a dictionary format.\n",
    "\n",
    "Below is the code to perform batch-wise inference and accumulate the prediction results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d807851-6dd4-45a3-b86d-ffa7e54e077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "dataloader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "# Calculate the total number of batches\n",
    "total_batches = len(dataloader)\n",
    "\n",
    "pred_results_ls = []\n",
    "processed_batches = 0\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}  # Move batch data to the appropriate device\n",
    "    with torch.no_grad():  # Inference mode: no gradient computation\n",
    "        outputs = model(**batch)\n",
    "    pred_results = evaluate_binary_classification_bert_build_pred_results(outputs['logits'], batch['labels'])\n",
    "    pred_results_ls.append(pred_results)  # Collecting prediction results\n",
    "    \n",
    "    processed_batches += 1  # Increment the count of processed batches\n",
    "    percent_complete = (processed_batches / total_batches) * 100  # Calculate the percentage of completion\n",
    "    print(f'Batch {processed_batches}/{total_batches} processed, {percent_complete:.2f}% complete.')  # Print the progress\n",
    "\n",
    "# Combine all batch results into one array for evaluation\n",
    "pred_results = np.concatenate(pred_results_ls)\n",
    "# Calculate and retrieve evaluation metrics\n",
    "eval_results, eval_results_ls = evaluate_binary_classification_bert(pred_results)\n",
    "\n",
    "# cleanup\n",
    "del model\n",
    "del batch\n",
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame([eval_results])\n",
    "# Set more meaningful index name\n",
    "results_df.index = ['Metrics']\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab1344-8d80-41bd-a3bd-3858499916f0",
   "metadata": {},
   "source": [
    "# Inference with the Phage Models\n",
    "\n",
    "In this section, we will walk through the evaluation of the phage test dataset, following a similar procedure to the previous example. The workflow includes:\n",
    "  * Preparing the model\n",
    "  * Preparing the dataset\n",
    "  * Evaluating the test set and measuring performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b52cd-72cf-45f7-a980-b6e67fd1cc1b",
   "metadata": {},
   "source": [
    "# Prearing the model: \n",
    "This is a simple model, that is trained using the 'MegatronBertForSequenceClassification' class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3daff-ac6f-4167-8ebf-dc0267cf9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MegatronBertForSequenceClassification\n",
    "\n",
    "finetuned_model = \"neuralbioinfo/prokbert-mini-phage\"\n",
    "kmer = 6\n",
    "shift= 1\n",
    "\n",
    "tok_params = {'kmer' : kmer,\n",
    "             'shift' : shift}\n",
    "tokenizer = ProkBERTTokenizer(tokenization_params=tok_params)\n",
    "model = MegatronBertForSequenceClassification.from_pretrained(finetuned_model)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "_=model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8ea5a-2092-4c1a-aec3-eb45d78d0770",
   "metadata": {},
   "source": [
    "## Preparing the Phage Dataset for Evaluation\n",
    "\n",
    "In this section, we load and prepare the phage dataset for evaluation. The dataset is a smaller subset suitable for testing, named \"phage-test-small\". We will demonstrate how to load this dataset, convert it to a pandas DataFrame.\n",
    "\n",
    "First, we load the dataset and set the batch size for the DataLoader, which controls how many samples will be processed simultaneously. Then, we convert the loaded dataset to a pandas DataFrame for easier manipulation and processing. Finally, we prepare the data for PyTorch by tokenizing and converting it into a format suitable for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9716080-e2f6-4751-ad40-fe6a68978236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nerualbioinfo/phage-test-small\")\n",
    "batch_size = 64\n",
    "\n",
    "# Loading and converting the dataset\n",
    "test_set = dataset[\"sample_test_L1024\"].to_pandas()\n",
    "print(f'Processing the database!')\n",
    "[X, y, torchdb] = get_torch_data_from_segmentdb_classification(tokenizer, test_set)\n",
    "dataset = ProkBERTTrainingDatasetPT(X, y, AddAttentionMask=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d46472-d40a-4226-b2a4-7bbdd197e070",
   "metadata": {},
   "source": [
    "## Inference and Evaluation of the Phage Dataset\n",
    "\n",
    "In this part of the process, we will perform inference on the phage dataset using the prepared DataLoader. The objective is to process each batch from the dataset, make predictions using the finetuned model, and then compile these predictions for evaluation.\n",
    "\n",
    "We initiate an empty list, `pred_results_ls`, to store the prediction results from each batch. We iterate over each batch in the DataLoader, ensuring that the batch data is moved to the same device as the model (GPU or CPU). With gradient computation disabled (to enhance performance and reduce memory usage during inference), we pass the batch through the model to obtain output logits.\n",
    "\n",
    "For each batch's output, we evaluate the binary classification results using the `evaluate_binary_classification_bert_build_pred_results` function, which processes the model's logits and the actual labels to generate prediction results. These results are then appended to our list.\n",
    "\n",
    "After processing all batches, we concatenate the list of arrays into a single numpy array. This aggregated result allows us to evaluate the overall performance of the model on the entire test set. We use the `evaluate_binary_classification_bert` function to calculate various evaluation metrics such as accuracy, precision, recall, F1 score, etc., based on the compiled prediction results. The `eval_results` dictionary will store these metrics for review and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c12f4-8757-4153-89d3-cd41b3679f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of batches\n",
    "total_batches = len(dataloader)\n",
    "\n",
    "pred_results_ls = []\n",
    "processed_batches = 0\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}  # Move batch data to the appropriate device\n",
    "    with torch.no_grad():  # Inference mode: no gradient computation\n",
    "        outputs = model(**batch)\n",
    "    pred_results = evaluate_binary_classification_bert_build_pred_results(outputs['logits'], batch['labels'])\n",
    "    pred_results_ls.append(pred_results)  # Collecting prediction results\n",
    "    \n",
    "    processed_batches += 1  # Increment the count of processed batches\n",
    "    percent_complete = (processed_batches / total_batches) * 100  # Calculate the percentage of completion\n",
    "    print(f'Batch {processed_batches}/{total_batches} processed, {percent_complete:.2f}% complete.')  # Print the progress\n",
    "\n",
    "# Combine all batch results into one array for evaluation\n",
    "pred_results = np.concatenate(pred_results_ls)\n",
    "# Calculate and retrieve evaluation metrics\n",
    "eval_results, eval_results_ls = evaluate_binary_classification_bert(pred_results)\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame([eval_results])\n",
    "# Set more meaningful index name\n",
    "results_df.index = ['Metrics']\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124c6d5-c83a-4367-abe6-4ce5bff1a318",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "In this evaluation, we have successfully executed the inference process using the pre-trained phage model and assessed its performance on the test dataset. Key metrics such as accuracy, F1 score, Matthews Correlation Coefficient (MCC), and Area Under the Curve (AUC) provide a comprehensive view of the model's ability to distinguish between classes.\n",
    "\n",
    "Additionally, while these results are promising, further validation and testing on independent datasets are recommended to ensure the model's generalizability and robustness. This could involve cross-validation, additional external datasets, or real-world applications.\n",
    "\n",
    "Finally, this notebook serves as a foundation for further exploration and refinement. Researchers are encouraged to experiment with different models, parameters, and datasets to enhance understanding and performance. The ultimate goal is to leverage these computational tools to advance our knowledge in microbiology and related fields.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
