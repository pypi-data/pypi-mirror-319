{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a8f50f-4e73-4bcf-abe9-51eccc6a5f0a",
   "metadata": {},
   "source": [
    "# Fine-tuning with the ProkBERT Model Family\n",
    "This notebook demonstrates how to utilize ProkBERT's pre-trained models for transfer learning tasks. We will apply the model to identify promoter sequences, framed as a binary classification problem where each segment is assigned a label.\n",
    "\n",
    "The main steps include:\n",
    "- Preparing the dataset to outline the labels for each segment.\n",
    "- Tokenizing nucleotide sequences.\n",
    "- Creating splits and PyTorch datasets.\n",
    "- Configuring training parameters such as learning rate, epochs, batch size, etc.\n",
    "- Training and evaluating the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ef551-0106-4329-85c3-82c3b47d2ebe",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "While ProkBERT can operate on CPUs, leveraging GPUs significantly accelerates the process. Google Colab offers free GPU usage (subject to time and memory limits), making it an ideal platform for trying and experimenting with ProkBERT models.\n",
    "\n",
    "## Enabling and testing the GPU (if you are using google colab)\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "- \n",
    "\n",
    "First, we'll install the ProkBERT package directly from its GitHub repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ff812-e2c5-494d-8066-3d996d0e5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProkBERT\n",
    "!pip install prokbert\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from prokbert.training_utils import get_default_pretrained_model_parameters, get_torch_data_from_segmentdb_classification\n",
    "from prokbert.models import BertForBinaryClassificationWithPooling\n",
    "from prokbert.prok_datasets import ProkBERTTrainingDatasetPT\n",
    "from prokbert.config_utils import ProkBERTConfig\n",
    "from prokbert.training_utils import compute_metrics_eval_prediction\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1b7e0-d7d0-4cc5-8ae8-feb356b64812",
   "metadata": {},
   "source": [
    "Next, we'll confirm that we can connect to the GPU with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5f172-42fe-45d4-ab19-e8d194aaca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'Found GPU at: {device_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41118b2c-376a-425d-b187-d6c768340420",
   "metadata": {},
   "source": [
    "## Sequence Data Preparation\n",
    "\n",
    "In this project, we will work with prokaryotic promoter sequences. The positive, known promoter sequences are derived from a prokaryotic promoter database. These sequences include the Transcription Start Site (TSS) located at position 60, with the promoter itself being an 80bp length sequence. Below is an illustration of the data structure we'll be working with.\n",
    "\n",
    "The data is labeled in a column named `label`, where `y=1` indicates a known promoter sequence, and `y=0` otherwise. It's crucial to ensure the sequence data is clean and segmented appropriately. This means verifying that it contains only nucleotide sequences, there are no empty sequences, etc.\n",
    "\n",
    "For detailed steps on sequence preprocessing, refer to the [segmentation notebook](https://github.com/nbrg-ppcu/prokbert/blob/main/examples/Segmentation.ipynb). For large-scale preprocessing, check out [this notebook](https://github.com/nbrg-ppcu/prokbert/blob/main/examples/Tokenization.ipynb).\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We'll start by loading a predefined dataset of bacterial promoters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22ed75-3271-419a-9b50-8634848e7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the predefined dataset\n",
    "dataset = load_dataset(\"neuralbioinfo/bacterial_promoters\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "test_sigma70_set = dataset[\"test_sigma70\"]\n",
    "multispecies_set = dataset[\"test_multispecies\"]\n",
    "\n",
    "\n",
    "train_db = train_set.to_pandas()\n",
    "test_sigma70_db = test_sigma70_set.to_pandas()\n",
    "test_ms_db = multispecies_set.to_pandas()\n",
    "\n",
    "train_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86dcee-c5c3-44d4-94c8-5a3b5c08e43f",
   "metadata": {},
   "source": [
    "## Loading the Pretrained Model\n",
    "\n",
    "At this stage, we will load the pretrained ProkBERT model from Hugging Face. For comprehensive details about the pretrained model and its architecture, please refer to the relevant documentation.\n",
    "\n",
    "Traditionally, models like `...SequenceClassification` classify sequences based on the hidden representation of the `[CLS]` or starting token. However, in our approach, we utilize the base model enhanced with a pooling layer that integrates information across all nucleotides in the sequence. The function `get_default_pretrained_model_parameters` is used here to load the model along with its corresponding tokenizer. It's crucial to ensure that the tokenizer's parameters, specifically LCA (Local Context Aware) tokenization settings, are aligned with those used during the model's pretraining phase. For our purposes, we adopt a k-mer size of 6 and a shift of 1.\n",
    "\n",
    "Here's how to load the ProkBERT model along with its tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5ef95-d99d-4f12-be01-bd2bb5786200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_path = 'neuralbioinfo/prokbert-mini'\n",
    "\n",
    "\n",
    "pretrained_model, tokenizer = get_default_pretrained_model_parameters(\n",
    "    model_name=model_name_path, \n",
    "    model_class='MegatronBertModel', \n",
    "    output_hidden_states=False, \n",
    "    output_attentions=False,\n",
    "    move_to_gpu=False\n",
    ")\n",
    "fine_tuned_model = BertForBinaryClassificationWithPooling(pretrained_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023de02c-6cf0-4d53-8c0a-84a317a761ff",
   "metadata": {},
   "source": [
    "## Tokenization and Dataset Creation\n",
    "\n",
    "In this phase, we proceed to tokenize the nucleotide sequences from our dataset. This process converts each sequence into a format that the ProkBERT model can understand and process. To ensure that our model pays attention only to meaningful tokens, we will pad the arrays and employ the `AddAttentionMask` flag. This flag helps the model distinguish between informative tokens and padding or non-informative tokens, allowing it to focus on relevant sequence parts during training and evaluation.\n",
    "\n",
    "### Creating Datasets\n",
    "\n",
    "We start by processing the training, testing, and validation datasets. Each database is tokenized using the ProkBERT tokenizer, and the resulting token arrays are prepared along with their corresponding labels. Here's a breakdown of the process for each dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a3d1f-5eba-4da6-984a-ad35e9ab8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Creating datasets!\n",
    "print(f'Processing train database!')\n",
    "[X_train, y_train, torchdb_train] = get_torch_data_from_segmentdb_classification(tokenizer, train_db)\n",
    "print(f'Processing test database!')\n",
    "[X_test, y_test, torchdb_test] = get_torch_data_from_segmentdb_classification(tokenizer, test_ms_db)\n",
    "print(f'Processing validation database!')\n",
    "[X_val, y_val, torchdb_val] = get_torch_data_from_segmentdb_classification(tokenizer, test_sigma70_db)\n",
    "\n",
    "train_ds = ProkBERTTrainingDatasetPT(X_train, y_train, AddAttentionMask=True)\n",
    "test_ds = ProkBERTTrainingDatasetPT(X_test, y_test, AddAttentionMask=True)\n",
    "val_ds = ProkBERTTrainingDatasetPT(X_val, y_val, AddAttentionMask=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b72345-c055-494f-94f6-44bcf397a7d6",
   "metadata": {},
   "source": [
    "## Training Configuration Setup\n",
    "\n",
    "We are setting up the configuration for fine-tuning a specific ProkBERT model. The configurations are divided into several categories to manage different aspects of the training process. Below is a brief overview of the parameters used in this example and their significance:\n",
    "\n",
    "### Model Parameters\n",
    "- **`model_outputpath`, `model_name`, `resume_or_initiation_model_path`**: We designate the model's output directory and name as `prokbert_mini_promoter`. These parameters ensure that the model's training outputs, including checkpoints, are saved under a specific directory named after the model. The model initiation path is also set to the same name, indicating where the model's initial weights are loaded from.\n",
    "- **`ResumeTraining`**: Set to `False` to start training from scratch rather than resuming previous training sessions.\n",
    "\n",
    "### Training Parameters\n",
    "- **`output_dir`**: Specifies the directory where training artifacts like model checkpoints will be saved. It combines a base directory `finetuned_models` with the model name.\n",
    "- **`warmup_steps`**: The number of warmup steps for the learning rate scheduler is set to 1, indicating minimal warmup before reaching the full learning rate.\n",
    "- **`save_steps` and `eval_steps`**: Both are set to 50, dictating how frequently the model should be saved and evaluated.\n",
    "- **`save_total_limit`**: Limits the total number of model checkpoints to keep to 10, helping manage storage efficiently.\n",
    "- **`learning_rate`**: The learning rate for fine-tuning is set at 0.0001.\n",
    "- **`per_device_train_batch_size`**: Defines the batch size for training as 128.\n",
    "- **`num_train_epochs`**: Specifies that the model will be trained for 1 epoch.\n",
    "- **`evaluation_strategy`**: Set to 'steps', indicating that evaluation will occur based on the number of steps defined.\n",
    "- **`per_device_eval_batch_size`**: The evaluation batch size is set to twice the training batch size, enhancing evaluation throughput.\n",
    "\n",
    "### ProkBERT Configuration\n",
    "\n",
    "These parameters are crucial for customizing the training process, allowing for specific training, evaluation strategies, and resource management tailored to the task and available computational resources. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f6aff-3c02-47ee-a468-062a8c47d5dc",
   "metadata": {},
   "source": [
    "## Configuration Parameters Overview\n",
    "\n",
    "The table below outlines the key configuration parameters for pretraining with ProkBERT, detailing their purpose, descriptions, default values, and types.\n",
    "\n",
    "| Section | Parameter | Description | Type | Default |\n",
    "|---------|-----------|-------------|------|---------|\n",
    "| **training** | | | | |\n",
    "| | `output_dir` | Output directory for training artifacts. | string | './train_output' |\n",
    "| | `num_train_epochs` | Total number of training epochs. | float | 1 |\n",
    "| | `save_steps` | Save model checkpoint every N steps. | integer | 1000 |\n",
    "| | `save_total_limit` | Maximum number of total checkpoints to keep. | integer | 20 |\n",
    "| | `logging_steps` | Log metrics every N steps. | integer | 50 |\n",
    "| | `logging_first_step` | Whether to log metrics for the first step. | boolean | True |\n",
    "| | `per_device_train_batch_size` | Batch size for training. | integer | 48 |\n",
    "| | `dataloader_num_workers` | Number of subprocesses for data loading. | integer | 1 |\n",
    "| | `learning_rate` | Learning rate for training. | float | 0.0005 |\n",
    "| | `adam_epsilon` | Epsilon for the Adam optimizer. | float | 5e-05 |\n",
    "| | `warmup_steps` | Number of warmup steps for learning rate scheduler. | integer | 500 |\n",
    "| | `weight_decay` | Weight decay for optimizer. | float | 0.1 |\n",
    "| | `adam_beta1` | Beta1 hyperparameter for the Adam optimizer. | float | 0.95 |\n",
    "| | `adam_beta2` | Beta2 hyperparameter for the Adam optimizer. | float | 0.98 |\n",
    "| | `gradient_accumulation_steps` | Number of steps to accumulate gradients before updating weights. | integer | 1 |\n",
    "| | `optim` | Optimizer to use for training. | string | \"adamw_torch\" |\n",
    "| | `ignore_data_skip` | Whether to ignore data skip or not. | boolean | True |\n",
    "| **dataset** | | | | |\n",
    "| | `dataset_path` | Path to the dataset. It triggers an error if empty. | string | '' |\n",
    "| | `pretraining_dataset_data` | The raw dataset data. | list | [[]] |\n",
    "| | `dataset_class` | The class of the dataset to be used. | string | 'IterableProkBERTPretrainingDataset' |\n",
    "| | `input_batch_size` | Batch size to be loaded into memory from the disk for HDF datasets. | int | 10000 |\n",
    "| | `dataset_iteration_batch_offset` | The offset value for dataset iteration start. | int | 0 |\n",
    "| | `max_iteration_over_dataset` | Maximum times to iterate over a dataset. | int | 10 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005ae0f-46bc-45c9-8800-e00d7ad778f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = 'prokbert_mini_promoter'\n",
    "default_ft_model_dir = 'finetuned_models'\n",
    "\n",
    "\n",
    "lr_rate = 0.0001\n",
    "batch_size = 128\n",
    "eval_steps = 50\n",
    "warmup_steps = 1\n",
    "num_train_epochs = 0.1\n",
    "\n",
    "\n",
    "model_params = {'model_outputpath': finetuned_model_name,\n",
    "                'model_name' : finetuned_model_name,\n",
    "                'resume_or_initiation_model_path' : finetuned_model_name, \n",
    "                'ResumeTraining' : False}\n",
    "dataset_params = {}\n",
    "training_params = {'output_dir': join(default_ft_model_dir, finetuned_model_name),\n",
    "                'warmup_steps' : warmup_steps,\n",
    "                'save_steps' : eval_steps,\n",
    "                'save_total_limit' : 10,\n",
    "                'learning_rate' : lr_rate,\n",
    "                'per_device_train_batch_size': batch_size,\n",
    "                'num_train_epochs': num_train_epochs,\n",
    "                'eval_steps' : eval_steps,\n",
    "                'logging_steps' : eval_steps,\n",
    "                'evaluation_strategy': 'steps',\n",
    "                'per_device_eval_batch_size': batch_size*2\n",
    "                }\n",
    "prokbert_config = ProkBERTConfig()\n",
    "prokbert_config.default_torchtype = torch.long\n",
    "\n",
    "_ = prokbert_config.get_and_set_model_parameters(model_params)\n",
    "_ = prokbert_config.get_and_set_dataset_parameters(dataset_params)\n",
    "_ = prokbert_config.get_and_set_pretraining_parameters(training_params)\n",
    "\n",
    "_ = prokbert_config.get_and_set_tokenization_parameters(tokenizer.tokenization_params)\n",
    "_ = prokbert_config.get_and_set_segmentation_parameters(tokenizer.segmentation_params)\n",
    "_ = prokbert_config.get_and_set_computation_params(tokenizer.comp_params)\n",
    "\n",
    "final_model_output = join(prokbert_config.model_params['model_outputpath'], prokbert_config.model_params['model_name'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35075e07-6a03-4cca-be8a-14a7b16862cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(**prokbert_config.pretraining_params)\n",
    "trainer = Trainer(\n",
    "                model=fine_tuned_model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_ds,\n",
    "                eval_dataset = val_ds,\n",
    "                compute_metrics=compute_metrics_eval_prediction,\n",
    "            )\n",
    "trainer.train()\n",
    "# Saving the final model\n",
    "print(f'Saving the model to: {final_model_output}')\n",
    "fine_tuned_model.save_pretrained(final_model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb5ada-6801-40ed-b79c-a07d50e0a139",
   "metadata": {},
   "source": [
    "# Fine-tuned Model\n",
    "\n",
    "The final fine-tuned model is available at the specified path, ready for deployment or further evaluation. While the current setup provides a good foundation, there's always room for improvement by experimenting with different hyperparameters. Fine-tuning these parameters can help greatly improve the model's performance on specific tasks or datasets.\n",
    "\n",
    "## Considerations for Further Optimization\n",
    "\n",
    "- **Experiment with Hyperparameters**: Adjust learning rate, batch size, number of epochs, and other training parameters to find the optimal configuration for your specific use case.\n",
    "- **Cross-validation**: Use cross-validation techniques to ensure that your model generalizes well across different subsets of your data.\n",
    "- **Data Augmentation**: Explore data augmentation strategies for sequence data, such as introducing random mutations or utilizing synthetic data generation, to increase the robustness of your model.\n",
    "- **Advanced Architectures**: Consider experimenting with different model architectures or integrating additional layers (i.e. convolution could be a good idea) to improve the model's capacity to capture complex patterns in the data.\n",
    "\n",
    "## Closing Remarks\n",
    "\n",
    "Fine-tuning a pre-trained model like ProkBERT offers a powerful approach to leveraging large language moels for biological sequence analysis. By carefully selecting and optimizing your model's hyperparameters, you can achieve significant improvements in performance. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
