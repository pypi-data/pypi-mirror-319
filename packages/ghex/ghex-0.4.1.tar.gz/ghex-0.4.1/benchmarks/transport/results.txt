MPI_THREAD_MULTIPLE

BASELINE: 1 thread

-M multi is important, because we want to use MT version of UCX. This adds overhead
time numactl --physcpubind=0-15 ucx_perftest -t tag_bw -M multi c3-4 -n 5000000 -s 1 -O 1

---------------------------------------------------------------------
niters			5000000
msg_size		1	1	1	1024	1024
inflight  		1	10	1000	1	10
---------------------------------------------------------------------

ucx_perftest		0.63	0.63		1.55	1.55

------------------------------
OpenMPI + HPCX 2.4
------------------------------
mpi_avail (waitany)   	1.40	2.10	80	1.78	2.51
mpi_avail (testany)   	0.86	0.98	13	1.63	1.57
mpi_avail_iter		0.82	0.85	1.18	1.83	1.71

------------------------------
Intel MPI  - even worse than the results below
------------------------------
mpi_avail (waitany)   	2.07	2.08	20	2.49	2.30
mpi_avail (testany)   	2.44	2.65	29	2.86	
mpi_avail_iter 		2.39	2.42	2.60	2.80	6.7

------------------------------
ghex_futures.cpp
------------------------------
MPI (ompi)  		0.85	0.86	1.21	1.59	1.75
MPI (intel)		
UCX			0.71	0.71	0.86	1.50	1.61

------------------------------
ghex_msg_cb.cpp
------------------------------
Sending a shared_message using callbacks.
n requests are submited and completed in turns.
Both backends can be used.

MPI (ompi)  		1.42	1.43	2.15	2.11	2.33
MPI (intel)		2.93	2.96	8.14	3.32	9.82
UCX			0.72	0.73	1.16	1.47	1.65
UCX (raw smsg)				1.04
UCX nbr/ghex progress	1.33	1.31	1.47	2.21	2.31

------------------------------
ghex_msg_cb_avail.cpp
------------------------------
Sending a shared_message using callbacks. There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
Both backends can be used.

MPI (ompi)  		1.48	1.43	2.38	2.26	2.32
MPI (intel)		3.01	3.05	-	3.42   13.10
UCX			0.85	0.73	1.17	1.65	1.50
UCX (raw smsg)				1.12
UCX nbr/ghex progress	1.34	1.31	1.52	2.31	2.29

------------------------------
ghex_msg_cb_resubmit.cpp
------------------------------
Sending a shared_message using callbacks. There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
recv requests are resubmited inside the recv callback.
Both backends can be used.

MPI (ompi)  		1.45	1.44	2.15	2.35	2.33
MPI (intel)		3.02	3.07	9.47	3.46   13.20	
UCX			0.83	0.73	1.15	1.48	1.56  (new MsgType overhead for large inflight)
UCX (raw smsg)				1.06
UCX nbr/ghex progress	1.38	1.32	1.55	2.22	2.38

------------------------------
ghex_msg_cb_dynamic.cpp
ghex_msg_cb_dynamic_resubmit.cpp
------------------------------
GHEX takes ownership of the message for the duration of the comm,
in user's code the message gets out of scope after posting.

UCX			1.50	1.40    1.80    2.71	2.52
UCX (raw smsg)		1.40	1.32	1.65	2.54	2.46
UCX (pool alloc circ)	1.24	1.17	3.17	1.92	1.81
UCX (pool, raw smsg)	1.15	1.06	2.67	1.80	1.79

------------------------------
ghex_ptr_cb.cpp
------------------------------
Sending buffer directly through pointer, using callbacks.
n requests are submited and completed in turns.
only UCX

UCX			0.71	0.72	1.06    1.61	1.58

------------------------------
ghex_ptr_cb_avail.cpp
------------------------------
Sending buffer directly through pointer, using callbacks.
There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
only UCX

UCX			0.83	0.72	1.07	1.61	1.52
