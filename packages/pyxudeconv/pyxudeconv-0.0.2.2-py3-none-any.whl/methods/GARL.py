# Goujon Accelerated Richardson-Lucy
import numpy as np

#from libraries import *
#import pyxu
from pyxu.operator import KLDivergence
import sys, os, glob

#sys.path.append('../mywc/')

import pyxu.abc as pxa
import pyxu.info.ptype as pxt
import torch
from pyxu.operator.interop import from_source
from pyxu.operator.interop import torch as pxotorch

from methods.models_GARL.wc_conv_net import WCvxConvNet, WCvxConvNet3DS, WCvxConvNet3D
from pathlib import Path
import json

#in future release, RRL will be incorporated in Pyxu
from methods.solver import RRL

from .ABC import HyperParametersDeconvolutionOptimizer
import importlib

#sys.path.append('./methods/configs/GARL')

__all__ = [
    "GARL",
]


class GARL(HyperParametersDeconvolutionOptimizer):
    r"""
    Hyper parameters optimizer for Goujon Accelerated Richardson-Lucy
    """

    def get_hyperparams(self):
        if '.json' in self._param_method[self._param_method.rfind('.'):]:
            with open(self._param_method, 'r', encoding="utf-8") as f:
                params = json.load(f)

            new_params = params.copy()
            for k in params.keys():
                if 'min' in k:
                    cp = k[:k.rfind('_min')]
                    new_params[cp] = np.linspace(
                        params[cp + '_min'],
                        params[cp + '_max'],
                        params[cp + '_nsteps'],
                    )
                    new_params.pop(cp + '_min')
                    new_params.pop(cp + '_max')
                    new_params.pop(cp + '_nsteps')
            params = new_params.copy()
        elif isinstance(self._param_method, dict):
            #Parameters are directly provided as a dictionary (possible if deconvolve is called from Python)
            params = self._param_method
        else:
            if not isinstance(self._param_method, str):
                #Load some default configurations in the package
                self._param_method = 'widefield_params' if len(
                    self._forw.codim_shape) == len(
                        self._forw.dim_shape) else 'airyscan_params'
            module_config = importlib.import_module('methods.configs.GARL.' +
                                                    self._param_method)
            config_fct = getattr(module_config, self._param_method)
            params = config_fct()
        return params

    def init_solver(self, param):
        device = torch.device(self._device_name)
        lossRL = KLDivergence(self._g)
        reg_shape = self._trim_buffer.codim_shape  #chooses where the regularization is enforced
        NN, applyNN, gradNN, proxNN = load_model_weakly_convex(
            param['model'],
            sigma=torch.tensor(param['sigma'], dtype=torch.float32).to(device),
            device=self._device_name,
            epoch=param['epochoi'],
            do3D=True,
            doSplineActivation=False,
        )
        NN.to(device)
        NN.conv_layer.spectral_norm(mode="power_method", n_steps=200)
        R = pxotorch.from_torch(dim_shape=reg_shape,
                                codim_shape=(1),
                                apply=applyNN,
                                cls=pxa.ProxDiffFunc,
                                grad=gradNN,
                                prox=None)
        R.lipschitz = NN.get_mu().maximum(
            torch.tensor(1)).detach().cpu().numpy()

        R = param['lmbd'] * R * self._trim_buffer
        self._solver = RRL(
            lossRL,
            self._forw,
            self._g,
            R,
            verbosity=self._disp,
            stop_rate=1,
            show_progress=False,
            bg=self._bg_est,
        )


def load_model_weakly_convex(
    name,
    sigma=None,
    device='cuda:0',
    epoch=None,
    do3D=False,
    doSplineActivation=True,
    #current_directory=None,
):
    directory_checkpoints = f'{name}checkpoints/'

    # retrieve last checkpoint
    if epoch is None:
        files = glob.glob(
            f'{directory_checkpoints}/checkpoints/*.pth',  #f'{current_directory}/trained_models/{name_glob}/checkpoints/*.pth',
            recursive=False)
        epochs = map(
            lambda x: int(x.split("/")[-1].split('.pth')[0].split('_')[1]),
            files)
        epoch = max(epochs)

    checkpoint_path = f'{directory_checkpoints}checkpoint_{epoch}.pth'
    # config file
    config = json.load(open(f'{name}config.json', encoding="utf-8"))

    # build model

    model, _ = build_modelWC(config, do3D, doSplineActivation)

    checkpoint = torch.load(checkpoint_path,
                            map_location=device,
                            weights_only=True)

    #breakpoint()
    model.to(device)

    model.load_state_dict(checkpoint['state_dict'])
    model.conv_layer.spectral_norm()
    model.eval()

    if 'cuda' in device:
        import cupy as xp
    else:
        import numpy as xp

    def arrange(arr: pxt.NDArray) -> pxt.NDArray:
        arr = arr.to(torch.float32)
        if do3D:
            if arr.ndim == 2:
                arr = arr.unsqueeze(0).unsqueeze(0).unsqueeze(
                    0)  #doesn't make sense
            elif arr.ndim == 3:
                arr = arr.unsqueeze(0)
            elif arr.ndim == 4:  #multichannels, but processed separately
                if arr.shape[-4] > 1:  #concatenate all channels in samples dim
                    arr = arr.reshape(-1, 1, *arr.shape[-3:])
        else:
            if arr.ndim == 2:
                arr = arr.unsqueeze(0).unsqueeze(0)
            elif arr.ndim == 3:
                if arr.shape[-3] > 1:  #expects one channel only
                    arr = arr.unsqueeze(1)
                else:
                    arr = arr.unsqueeze(0)
            elif arr.ndim == 4:
                if arr.shape[-3] > 1:  #concatenate all in samples dim
                    arr = arr.reshape(-1, 1, *arr.shape[2:])
        return arr

    def applyNN(arr: pxt.NDArray) -> pxt.NDArray:
        with torch.no_grad():
            out = model.cost(arrange(arr), sigma=sigma)
            return out.sum()

    def gradNN(arr: pxt.NDArray) -> pxt.NDArray:
        with torch.no_grad():
            shape_og = arr.shape
            return model.grad(arrange(arr), sigma=sigma).reshape(shape_og)

    def proxNN(arr: pxt.NDArray, rho: pxt.Real) -> pxt.NDArray:
        with torch.no_grad():
            return model.forward(arrange(arr))  #ToDo

    return model, applyNN, gradNN, proxNN


def build_modelWC(config, do3D, doSplineActivation):
    # ensure consistency of the config file, e.g. number of channels, ranges + enforce constraints
    if doSplineActivation:
        # 1- Activation function (learnable spline)
        param_spline_activation = config['spline_activation']
        # non expansive increasing splines
        param_spline_activation["slope_min"] = 0
        param_spline_activation["slope_max"] = 1
        # antisymmetric splines
        param_spline_activation["antisymmetric"] = True
        # shared spline
        param_spline_activation["num_activations"] = 1

    # 2- Multi convolution layer
    param_multi_conv = config['multi_convolution']
    if len(param_multi_conv['num_channels']) != (
            len(param_multi_conv['size_kernels']) + 1):
        raise ValueError(
            "Number of channels specified is not compliant with number of kernel sizes"
        )

    param_spline_scaling = config['spline_scaling']
    param_spline_scaling["clamp"] = False
    param_spline_scaling["x_min"] = config['noise_range'][0]
    param_spline_scaling["x_max"] = config['noise_range'][1]
    param_spline_scaling["num_activations"] = config['multi_convolution'][
        'num_channels'][-1]

    if do3D:
        if doSplineActivation:
            model = WCvxConvNet3D(
                param_multi_conv=param_multi_conv,
                param_spline_activation=param_spline_activation,
                param_spline_scaling=param_spline_scaling,
                rho_wcvx=config['rho_wcvx'])
        else:
            model = WCvxConvNet3DS(param_multi_conv=param_multi_conv,
                                   param_spline_scaling=param_spline_scaling,
                                   rho_wcvx=config['rho_wcvx'])
    else:
        model = WCvxConvNet(param_multi_conv=param_multi_conv,
                            param_spline_activation=param_spline_activation,
                            param_spline_scaling=param_spline_scaling,
                            rho_wcvx=config['rho_wcvx'])

    return (model, config)
