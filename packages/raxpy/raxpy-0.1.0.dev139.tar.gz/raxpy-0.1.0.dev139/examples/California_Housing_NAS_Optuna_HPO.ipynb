{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Nueral Network Architecture and Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Neural Architecture Search (NAS)\n",
    "\n",
    "The following demonstrates the training within an annotated function to support NAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Annotated, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import raxpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "# Load the California Housing Dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "@dataclass\n",
    "class Layer:\n",
    "    dropout:Annotated[float, raxpy.Float(lb=0.0, ub=0.9)]\n",
    "    neuron_count:Annotated[int, raxpy.Integer(lb=8, ub=256)]\n",
    "    activation_type:bool\n",
    "\n",
    "    def create_layer(self, input_shape):\n",
    "        parts = [\n",
    "            Dense(self.neuron_count, activation='relu', input_shape=input_shape),\n",
    "        ]\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            parts.append(Dropout(self.dropout))  # Dropout for regularization\n",
    "        return parts\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LearningRateScheduleFixed:\n",
    "    learning_rate:Annotated[float, raxpy.Float(lb=0.0001,ub=0.01)] = 0.001\n",
    "\n",
    "    def create(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LearningRateScheduleExponentialDecay:\n",
    "    initial_learning_rate:Annotated[float, raxpy.Float(lb=0.0001,ub=0.01)] = 0.001\n",
    "    decay_steps:Annotated[float, raxpy.Integer(lb=100,ub=10000)]=1000,           # Decay every 1000 steps\n",
    "    decay_rate:Annotated[float, raxpy.Float(lb=0.5,ub=0.99)]=0.96,            # Multiply by 0.96 at each decay step\n",
    "    staircase:bool=True              # If True, decay in discrete steps\n",
    "\n",
    "    def create(self):\n",
    "        return tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=self.initial_learning_rate,\n",
    "            decay_steps=self.decay_steps,\n",
    "            decay_rate=self.decay_rate,\n",
    "            staircase=self.staircase\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerSGD:\n",
    "    learning_rate_schedule:Union[LearningRateScheduleFixed, LearningRateScheduleExponentialDecay] = field(default_factory=LearningRateScheduleFixed)\n",
    "    momentum:Annotated[float, raxpy.Float(lb=0.5,ub=0.95)] = 0.9       # Momentum factor for smoother convergence\n",
    "    nesterov:bool = True       # Nesterov momentum for accelerated learning\n",
    "\n",
    "    \n",
    "    def create(self):\n",
    "        return SGD(\n",
    "            learning_rate=self.learning_rate_schedule.create(),\n",
    "            momentum=self.momentum,\n",
    "            nesterov=self.nesterov\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerADAM:\n",
    "    beta_1:Annotated[float, raxpy.Float(lb=0.8,ub=0.99)]=0.9           # Exponential decay rate for the 1st moment (mean of gradients)\n",
    "    beta_2:Annotated[float, raxpy.Float(lb=0.8,ub=0.9999)]=0.999         # Exponential decay rate for the 2nd moment (variance of gradients)\n",
    "    amsgrad:bool=False\n",
    "    learning_rate_schedule:Union[LearningRateScheduleFixed, LearningRateScheduleExponentialDecay] = field(default_factory=LearningRateScheduleFixed)\n",
    "\n",
    "    def create(self):\n",
    "        return Adam(\n",
    "            learning_rate=self.learning_rate_schedule.create(),\n",
    "            beta_1=self.beta_1,\n",
    "            beta_2=self.beta_2,\n",
    "            epsilon=1e-07,# Small constant to prevent division by zero\n",
    "            amsgrad=self.amsgrad\n",
    "        )\n",
    "\n",
    "\n",
    "def train(\n",
    "    batch_size:Annotated[int, raxpy.Integer(value_set=[8,16,32,64])],\n",
    "    scaler_flag:bool,\n",
    "    layer_1:Layer,\n",
    "    layer_2:Layer,\n",
    "    layer_3:Optional[Layer],\n",
    "    layer_4:Optional[Layer],\n",
    "    optimizer:Union[OptimizerADAM,OptimizerSGD]\n",
    "):\n",
    "    # Standardize the data (important for neural networks)\n",
    "\n",
    "    if scaler_flag:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    X_train_t = scaler.fit_transform(X_train)\n",
    "    X_test_t = scaler.transform(X_test)\n",
    "\n",
    "    c = layer_1.create_layer((X_train_t.shape[1],)) + layer_2.create_layer((layer_1.neuron_count,))\n",
    "    p_count = layer_2.neuron_count\n",
    "    if layer_3 is not None:\n",
    "        c = c + layer_3.create_layer((p_count,))\n",
    "        p_count = layer_3.neuron_count\n",
    "    if layer_4 is not None:\n",
    "        c = c + layer_4.create_layer((p_count,))\n",
    "        p_count = layer_4.neuron_count\n",
    "\n",
    "    c = c + [\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ]\n",
    "    # Define the ANN model\n",
    "    model = Sequential(c)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer.create(), loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "    # Set up early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',       # Monitor validation loss\n",
    "        patience=5,              # Stop after 5 epochs with no improvement\n",
    "        restore_best_weights=True # Restore model weights from the best epoch\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_t, y_train, validation_split=0.2, callbacks=[early_stopping], epochs=250, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, mae = model.evaluate(X_test_t, y_test, verbose=0)\n",
    "    print(f\"Test Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    # Predict and calculate the Mean Squared Error\n",
    "    y_pred = model.predict(X_test_t)\n",
    "    try:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"Test Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        # Plot training history\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        mse = np.inf\n",
    "        print(f\"Invalid Test Mean Squared Error (MSE)\")\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Example\n",
    "\n",
    "The following demonstrates using raxpy with Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raxpy.adapters.optuna as raxpy_optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(\n",
    "        init_sampling_points:Annotated[int, raxpy.Integer(lb=0, ub=50)],\n",
    "        designer,\n",
    "        max_points=100,\n",
    "        f=train,\n",
    "    ):\n",
    "    study = optuna.create_study()\n",
    "\n",
    "    if init_sampling_points > 0:\n",
    "        input_space = raxpy.function_spec.extract_input_space(f)\n",
    "        design = designer(input_space, init_sampling_points)\n",
    "        raxpy_optuna.enqueue_trials_from_doe_to_study(design, study)\n",
    "\n",
    "    fn = raxpy_optuna.convert_to_optuna(f)\n",
    "\n",
    "    study.optimize(fn, n_trials=max_points)\n",
    "\n",
    "    return (study.best_params, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_point_count = 10\n",
    "exploit_point_count = 10\n",
    "best_point, hp_f = optimize(init_sampling_points=explore_point_count, designer=raxpy.generate_design, f=train, max_points=exploit_point_count+explore_point_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optmize_f(\n",
    "    init_sample_points:Annotated[float, raxpy.Integer(lb=10, ub=100)],\n",
    "    opt_points:Annotated[float, raxpy.Integer(lb=5, ub=100)],\n",
    "):\n",
    "\n",
    "    max_points = init_sample_points + opt_points\n",
    "    point_1,fn = optimize(init_sampling_points=init_sample_points, designer=raxpy.generate_design, f=train, max_points=opt_points)\n",
    "    point_2,fn = optimize(init_sampling_points=init_sample_points, designer=raxpy.generate_random_design, f=train, max_points=opt_points)\n",
    "    point_3,fn = optimize(init_sampling_points=0, designer=raxpy.generate_random_design, f=train, max_points=max_points)\n",
    "\n",
    "    p1_b = fn(point_1)\n",
    "    p2_b = fn(point_2)\n",
    "    p3_b = fn(point_3)\n",
    "\n",
    "    return p1_b, p2_b, p3_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
