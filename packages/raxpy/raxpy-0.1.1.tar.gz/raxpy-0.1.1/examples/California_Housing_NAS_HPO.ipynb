{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Nueral Network Architecture and Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing Dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the ANN model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Set up early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',       # Monitor validation loss\n",
    "    patience=10,              # Stop after 10 epochs with no improvement\n",
    "    restore_best_weights=True # Restore model weights from the best epoch\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,              # Maximum epochs\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping], # Add early stopping\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# Predict and calculate the Mean Squared Error\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Neural Architecture Search (NAS)\n",
    "\n",
    "The following demonstrates the training within an annotated function to support NAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Annotated, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import raxpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "# Load the California Housing Dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "@dataclass\n",
    "class Layer:\n",
    "    dropout:Annotated[float, raxpy.Float(lb=0.0, ub=0.9)]\n",
    "    neuron_count:Annotated[int, raxpy.Integer(lb=8, ub=256)]\n",
    "    activation_type:bool\n",
    "\n",
    "    def create_layer(self, input_shape):\n",
    "        parts = [\n",
    "            Dense(self.neuron_count, activation='relu', input_shape=input_shape),\n",
    "        ]\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            parts.append(Dropout(self.dropout))  # Dropout for regularization\n",
    "        return parts\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LearningRateScheduleFixed:\n",
    "    learning_rate:Annotated[float, raxpy.Float(lb=0.0001,ub=0.01)] = 0.001\n",
    "\n",
    "    def create(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LearningRateScheduleExponentialDecay:\n",
    "    initial_learning_rate:Annotated[float, raxpy.Float(lb=0.0001,ub=0.01)] = 0.001\n",
    "    decay_steps:Annotated[float, raxpy.Integer(lb=100,ub=10000)]=1000,           # Decay every 1000 steps\n",
    "    decay_rate:Annotated[float, raxpy.Float(lb=0.5,ub=0.99)]=0.96,            # Multiply by 0.96 at each decay step\n",
    "    staircase:bool=True              # If True, decay in discrete steps\n",
    "\n",
    "    def create(self):\n",
    "        return tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=self.initial_learning_rate,\n",
    "            decay_steps=self.decay_steps,\n",
    "            decay_rate=self.decay_rate,\n",
    "            staircase=self.staircase\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerSGD:\n",
    "    learning_rate_schedule:Union[LearningRateScheduleFixed, LearningRateScheduleExponentialDecay] = field(default_factory=LearningRateScheduleFixed)\n",
    "    momentum:Annotated[float, raxpy.Float(lb=0.5,ub=0.95)] = 0.9       # Momentum factor for smoother convergence\n",
    "    nesterov:bool = True       # Nesterov momentum for accelerated learning\n",
    "\n",
    "    \n",
    "    def create(self):\n",
    "        return SGD(\n",
    "            learning_rate=self.learning_rate_schedule.create(),\n",
    "            momentum=self.momentum,\n",
    "            nesterov=self.nesterov\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerADAM:\n",
    "    beta_1:Annotated[float, raxpy.Float(lb=0.8,ub=0.99)]=0.9           # Exponential decay rate for the 1st moment (mean of gradients)\n",
    "    beta_2:Annotated[float, raxpy.Float(lb=0.8,ub=0.9999)]=0.999         # Exponential decay rate for the 2nd moment (variance of gradients)\n",
    "    amsgrad:bool=False\n",
    "    learning_rate_schedule:Union[LearningRateScheduleFixed, LearningRateScheduleExponentialDecay] = field(default_factory=LearningRateScheduleFixed)\n",
    "\n",
    "    def create(self):\n",
    "        return Adam(\n",
    "            learning_rate=self.learning_rate_schedule.create(),\n",
    "            beta_1=self.beta_1,\n",
    "            beta_2=self.beta_2,\n",
    "            epsilon=1e-07,# Small constant to prevent division by zero\n",
    "            amsgrad=self.amsgrad\n",
    "        )\n",
    "\n",
    "\n",
    "def train(\n",
    "    batch_size:Annotated[int, raxpy.Integer(value_set=[8,16,32,64])],\n",
    "    scaler_flag:bool,\n",
    "    layer_1:Layer,\n",
    "    layer_2:Layer,\n",
    "    layer_3:Optional[Layer],\n",
    "    layer_4:Optional[Layer],\n",
    "    optimizer:Union[OptimizerADAM,OptimizerSGD]\n",
    "):\n",
    "    # Standardize the data (important for neural networks)\n",
    "\n",
    "    if scaler_flag:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    X_train_t = scaler.fit_transform(X_train)\n",
    "    X_test_t = scaler.transform(X_test)\n",
    "\n",
    "    c = layer_1.create_layer((X_train_t.shape[1],)) + layer_2.create_layer((layer_1.neuron_count,))\n",
    "    p_count = layer_2.neuron_count\n",
    "    if layer_3 is not None:\n",
    "        c = c + layer_3.create_layer((p_count,))\n",
    "        p_count = layer_3.neuron_count\n",
    "    if layer_4 is not None:\n",
    "        c = c + layer_4.create_layer((p_count,))\n",
    "        p_count = layer_4.neuron_count\n",
    "\n",
    "    c = c + [\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ]\n",
    "    # Define the ANN model\n",
    "    model = Sequential(c)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer.create(), loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "    # Set up early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',       # Monitor validation loss\n",
    "        patience=15,              # Stop after 15 epochs with no improvement\n",
    "        restore_best_weights=True # Restore model weights from the best epoch\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_t, y_train, validation_split=0.2, callbacks=[early_stopping], epochs=250, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, mae = model.evaluate(X_test_t, y_test, verbose=0)\n",
    "    print(f\"Test Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    # Predict and calculate the Mean Squared Error\n",
    "    y_pred = model.predict(X_test_t)\n",
    "    try:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"Test Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        # Plot training history\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        mse = np.inf\n",
    "        print(f\"Invalid Test Mean Squared Error (MSE)\")\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function defined above like any other Python function. This function trains an artifical neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(32,False,Layer(0.2,256,False),Layer(0.5,32,False), None, None, OptimizerADAM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(32,False,Layer(0.2,256,False),Layer(0.2,64,False), Layer(0.0,32,False), None, OptimizerSGD())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the function to conduct a experiment using a space-filling design to explore the affects of hyper-parameters and network architecture settings on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = raxpy.perform_experiment(train,n_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2, outputs2 = raxpy.perform_experiment(train,n_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt Example\n",
    "\n",
    "The following demonstrates using raxpy with HyperOpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raxpy.adapters.hyperopt as rhp\n",
    "from hyperopt import fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(\n",
    "        init_sampling_points:Annotated[int, raxpy.Integer(lb=0, ub=50)],\n",
    "        designer,\n",
    "        max_points=100,\n",
    "        f=train,\n",
    "    ):\n",
    "\n",
    "    if init_sampling_points > 0:\n",
    "        input_space = raxpy.function_spec.extract_input_space(f)\n",
    "        design = designer(input_space, init_sampling_points)\n",
    "        inputs = rhp.convert_design(design)\n",
    "    else:\n",
    "        inputs = []\n",
    "\n",
    "    space, fn = rhp.convert_to_hp(f)\n",
    "\n",
    "    best = fmin(\n",
    "        fn=fn,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_points,\n",
    "        points_to_evaluate=inputs,\n",
    "    )\n",
    "\n",
    "    return (best, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_point_count = 10\n",
    "exploit_point_count = 10\n",
    "best_point, hp_f = optimize(init_sampling_points=explore_point_count, designer=raxpy.generate_design, f=train, max_points=exploit_point_count+explore_point_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optmize_f(\n",
    "    init_sample_points:Annotated[float, raxpy.Integer(lb=10, ub=100)],\n",
    "    opt_points:Annotated[float, raxpy.Integer(lb=5, ub=100)],\n",
    "):\n",
    "\n",
    "    max_points = init_sample_points + opt_points\n",
    "    point_1,fn = optimize(init_sampling_points=init_sample_points, designer=raxpy.generate_design, f=train, max_points=opt_points)\n",
    "    point_2,fn = optimize(init_sampling_points=init_sample_points, designer=raxpy.generate_random_design, f=train, max_points=opt_points)\n",
    "    point_3,fn = optimize(init_sampling_points=0, designer=raxpy.generate_random_design, f=train, max_points=max_points)\n",
    "\n",
    "    p1_b = fn(point_1)\n",
    "    p2_b = fn(point_2)\n",
    "    p3_b = fn(point_3)\n",
    "\n",
    "    return p1_b, p2_b, p3_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
